\section{Results}
\label{sec:results}
%(Sam Schmidt, Bryce Kalmbach, Johann Cohen Tanugi, Rongpu Zhou, Alex Malz)

We begin with a demonstrative visual inspection of the \pzpdf s produced by each code for individual galaxies.
Figure~\ref{fig:pz_examples} shows the \pzpdf s for four galaxies chosen as examples of \pzpdf\ archetypes: a narrow unimodal redshift solution, a broad unimodal solution, a bimodal distribution, and a multimodal distribution.

\begin{figure*}
%\centering
\includegraphics[width=0.49\textwidth]{fig/pz_12codes_261931_noseaborn_crop.jpg}\includegraphics[width=0.49\textwidth]{fig/pz_12codes_471167_noseaborn_crop.jpg}\\
\includegraphics[width=0.49\textwidth]{fig/pz_12codes_713178_noseaborn_crop.jpg}\includegraphics[width=0.49\textwidth]{fig/pz_12codes_982747_noseaborn_crop.jpg}
\caption{The individual \pzpdf s (blue) distributions produced by the twelve codes (small panels) on four exemplary galaxies' photometry (large panels) with different true redshifts (red).
The \pzpdf s of all codes share some features for the example galaxies due to physical color degeneracies and photometric errors: tight unimodal $p(z)$ (upper left), broad unimodal $p(z)$ (upper right), bimodal $p(z)$ (lower right), and complex/multimodal $p(z)$ (lower left).
The diverse algorithms and implementations induce differences in small-scale structure and sensitivity to physical systematics.}
\label{fig:pz_examples}
\end{figure*}

The most striking differences between codes are due to small-scale features induced by the interaction between the shared piecewise constant parameterization of $200$ bins $0 < z < 2$ of Section~\ref{sec:metrics} and the smoothing conditions or lack thereof in each algorithm.
The $\rm{d}z = 0.01$ redshift resolution is sufficient to capture the broad peaks of faint galaxies' \pzpdf s with large photometric errors but is too broad to resolve the narrow peaks for bright galaxies' \pzpdf s with small photometric errors.
This observation is consistent with the findings of \citet[]{Malz:qp} that the piecewise constant parameterization underperforms in the presence of small-scale structures.
However, the shared small-scale features of \annz, \metaphor, \cmnn, and \skynet\ are a result of various weighted sums of the limited number of training set galaxies with colors similar to those of the test set galaxy in question, with behavior closer to classification than regression in the case of \annz.
The settings used on \gpz\ in this work forced broadening of the single Gaussian to cover the multimodal redshift solutions of the other codes.
%Interestingly, while \textsc{ANNz2} shows an abundance of small scale structure in individual $p(z)$ measurements (see Fig.~\ref{fig:pz_examples}), the summed $\hat{N}(z)$ is rather smooth, where the small scale features average out.  This is not the case for the two other codes that show an abundance of substructure in their individual $p(z)$: both \textsc{CMNN} and \textsc{SkyNet} show small scale features both in $p(z)$ and $\hat{N}(z)$.
%In contrast, FlexZBoost, for example, can return estimates on any grid without compression errors as itâ€™s a basis expansion method where only the expansion coefficients need to be stored.
%Codes with a native output format other than the shared piecewise constant binning scheme (or one that can be losslessly converted to it) may suffer from loss of information when converting to it, which could artificially favor some codes over others in a limited number of cases, for example bright galaxies with very narrow $p(z)$ where the true peak falls between grid points.  We will discuss PDF storage in Section~\ref{sec:discussion}.
%Furthermore, the fidelity of photo-$z$ interim posteriors in this format varies with the quality of the photometry.
%Switching to a quantile based parameterization may be more costly computationally, for example template-based codes would need to test more grid point in order to resolve the quantiles for bright galaxies.  However, the computational time for template based codes scales roughly linearly with the number of grid points, so this may be a reasonable thing to implement.
% We will discuss this further in Section~\ref{sec:discussion}.
% \red{someone review this statement to make sure that I'm saying this correctly!}

\subsection{Performance on \pzpdf\ ensembles}
\label{sec:pitqq}

Figure~\ref{fig:pitqq} shows a histogram of PIT values, QQ plot, and QQ difference plot relative to the ideal diagonal, showcasing the biases and trends in the average accuracy of the \pzpdf s for each code.
The high QQ values (more high than low PIT values) of \bpz, \cmnn, \delight, \eazy, and \gpz\ indicate \pzpdf s biased low, and the low QQ values (more low than high PIT values) of \skynet\ and \tpz\ indicate \pzpdf s biased high.

\begin{figure*}
\centering
\includegraphics[width=0.74\textwidth]{fig/PITANDQQplot_12codes_crop.jpg}
\caption{The QQ plot (red) and PIT histogram (blue) of the \pzpdf\ codes (panels) along with the ideal QQ (black dashed diagonal) and ideal PIT (gray horizontal) curves, as well as a difference plot for the QQ difference from the ideal diagonal (lower inset).
The twelve codes exhibit varying degrees of four deviations from perfection: an overabundance of PIT values at the centre of the distribution indicate a catalogue of overly broad \pzpdf s, an excess of PIT values at the extrema indicates a catalogue of overly narrow \pzpdf s, catastrophic outliers manifest as overabundances at PIT values of 0 and 1, and asymmetry indicates systematic bias, a form of model misspecification.}
\label{fig:pitqq}
\end{figure*}

The PIT histograms of \delight, \cmnn, \skynet, and \tpz\ feature an underrepresentation of extreme values, indicative of overly broad \pzpdf s, while the overrepresentation of extreme values for for \metaphor indicate overly narrow \pzpdf s.
These five codes in particular have a free parameter for bandwidth, which may be responsible for this vulnerability, in spite of the opportunity for fine-tuning with perfect prior information.
\flexzboost's ``sharpening'' parameter (described in Section~\ref{sec:flexzboost}) played a key role in diagonalizing the QQ plot, indicating a common avenue for improvement in the approaches that share this type of parameter.
On the other hand, the three purely template-based codes, \bpz, \eazy, and \lephare, do not exhibit much systematic broadening or narrowing, which may indicate that complete template coverage effectively defends from these effects.

\begin{table}
\setlength{\tabcolsep}{2pt}
\centering
\caption{The catastrophic outlier rate as defined by extreme PIT values.
We expect a value of 0.0002 for a proper Uniform distribution.
An excess over this small value indicates true redshifts that fall outside the non-zero support of the $p(z)$.}
\label{tab:pitoutlier}
\begin{tabular}{lc}
\hline
\hline
\Pz\ Code & fraction PIT$<10^{-4}$ or $>$0.9999\\
\hline
\annz       & 0.0265\\
\bpz        & 0.0192\\
\delight    & 0.0006\\
\eazy       & 0.0154\\
\flexzboost & 0.0202\\
\gpz        & 0.0058\\
\lephare    & 0.0486\\
\metaphor   & 0.0229\\
\cmnn       & 0.0034\\
\skynet     & 0.0001\\
\tpz        & 0.0130\\
\hline
\trainz     & 0.0002\\
\end{tabular}
\end{table}

Though the spikes in the first and last bin of the PIT histogram were cut off in Figure~\ref{fig:pitqq} for visualization, the catastrophic outlier rates are provided in Table~\ref{tab:pitoutlier}.
As expected, \trainz\ achieves precisely the 0.0002 value expected of an ideal PIT distribution.
\annz, \flexzboost, \lephare, and \metaphor\ have notably high catastrophic outlier rates $> 0.02$, exceeding 100 times the ideal PIT rate, meriting further investigation.

\begin{figure*}
\centering
\includegraphics[width=0.74\textwidth]{fig/KSvsCvMvsAD_PIT_withnull_jpg.jpg}
\caption{A visualization of the Kolmogorov-Smirnoff (KS, blue diamond), Cramer-von Mises (CvM, black star), and Anderson-Darling (AD, red asterisk) statistics for the PIT distributions.
There is generally good agreement between these statistics, with differences corresponding to the codes with outstanding catastrophic outlier rates, a reflection in the differences in how each statistic weights the tails of the distribution.}
\label{fig:pit_stats}
\end{figure*}

Figure~\ref{fig:pit_stats} displays the values of the KS, CvM, and AD test statistics between the PIT distribution and a uniform distribution $U(0, 1)$, highlighting the relative rather than absolute numbers.
%\red{Can p-values be supplied for each statistic? The statistics themselves are difficult to interpret, other than ``lower is better'' (p-value in skgof was broken, having trouble finding 1-sample KS calculation for uniform distribution)}
\metaphor\ and \lephare\ perform well under the AD but poorly under the KS and CvM due to their high catastrophic outlier rates.
\annz\ and \flexzboost\  are the top scorers under these metrics of the PIT distribution.
\annz's strong performance can be attributed to an aspect of the training process in which training set galaxies with a PIT that more closely matches the percentiles of the DC1 training set's redshift distribution are upweighted; in effect, these quantile-based metrics were part of the algorithm itself that may or may not serve it well under more realistic experimental conditions.

\subsection{Performance on the stacked estimator of the redshift distribution}
\label{sec:stackedmetrics}

Figure~\ref{fig:nz} shows the stacked estimator $\hat{N}(z)$ of the redshift distribution for each code compared to the true redshift distribution $\tilde{N}(z)$, where the stacked estimator has been smoothed for each code in the plot using a kernel density estimate (KDE) with a bandwidth chosen by Scott's Rule \citep{Scott:1992} in order to minimize visual differences in small-scale features; the quantiative statistics, however, are calculated using the empirical CDF which is not smoothed.

\begin{figure*}
\centering
\includegraphics[width=0.74\textwidth]{fig/NZsumplot_12codes_scottsrule.jpg}
\caption{The smoothed stacked estimator $\hat{N}(z)$ of the redshift distribution (red) produced by each code (panels) compared to the true redshift distribution $\tilde{N}(z)$ (blue).
Varying levels of agreement are seen among the codes, with the smallest deviations for \cmnn, \flexzboost, \tpz, and \trainz.}
\label{fig:nz}
%\aim{Arguably this could also be clarified by showing the true $n'(z)$ once and then differences from it for each cod's stacked estimator\dots}\scc{what if we plot the difference between true and stacked rather than soverposing the two lines? Sam please do not hate me.}
\end{figure*}

Many of the codes, including all the model-fitting approaches and \annz, \gpz, \metaphor, and \skynet\ from the data-driven camp, overestimate the redshift density at $z \sim 1.4$.
This behavior is a consequence of the $4000\ {\rm \AA}$ break passing through the gap between the $z$ and $y$ filters, which induces a genuine discontinuity in the $z - y$ colour as a function of redshift that can sway the \pzpdf\ estimates in the absence of bluer spectral features.

\annz, \gpz, and \metaphor\ show signs of overtraining, estimating enhanced peaks and diminished troughs relative to the training set, an obstacle that may be overcome with adjustment of the implementation.

As expected, \trainz\ perfectly recovers the true redshift distribution: as the training sample is selected from the same underlying distribution as the test set, the redshift distributions are identical, up to Poisson fluctuations due to the finite number of sample galaxies.
\cmnn\ is also in excellent agreement for similar reasons: with a representative training sample of galaxies spanning the colour-space, the sum of the colour-matched neighbour redshifts should return the true redshift distribution.
\flexzboost\ and \tpz\ also perform superb recovery of the true redshift distribution, with only a slight deviation at $z \sim 1.4$.
Our metrics, however, cannot discern whether these four approaches, as well as \delight, are spared the $z \sim 1.4$ degeneracy in $\hat{N}(z)$ because they have more effectively used information in the data or if the impact is simply washed out by the stacked estimator's effective average over the test set galaxy sample.
See Appendix~\ref{sec:pointmetrics} for further discussion of the $z \sim 1.4$ issue.

Figure~\ref{fig:nz_stats} shows the quantitative Kolmogorov-Smirnoff (KS), Cramer-Von Mises (CvM), and Anderson Darling (AD) test statistics for each of the codes for the $\hat{N}(z)$ based measures.
%\red{Can p-values be supplied for each statistic? The statistics themselves are difficult to interpret, other than ``lower is better'' (no, p-values are very difficult to compute for non-uniform distributions)}
The stacked estimators of the redshift distribution for \cmnn\ and \trainz\ best estimate $\tilde{N}(z)$ under these metrics, and the only codes that do especially poorly are \eazy, \lephare, \metaphor, and \skynet.
It is unsurprising that \cmnn\ scores well, as with a near perfectly representative training set means that choosing neighbouring points in color/magnitude space should lead to excellent agreement in the final $\hat{N}(z)$ estimate.

\begin{figure*}
\centering
\includegraphics[width=0.74\textwidth]{fig/KSvsCvMvsAD_NZ_withnull_jpg.jpg}
\caption{A visualization of the Kolmogorov-Smirnoff (KS, blue diamond), Cramer-von Mises (CvM, black star), and Anderson-Darling (AD, red asterisk) statistics for the $\hat{N}(z)$ distributions.
We make the reassuring observation that these related statistics do not disagree significantly with one another.
\cmnn\ performs comparably well to \trainz, the control case, while \skynet\ scores poorly due to an overall bias in its redshift predictions.}
\label{fig:nz_stats}
\end{figure*}

It is, however, surprising that \tpz\ does well on $\hat{N}(z)$ given its poor performance on the ensemble \pzpdf s, especially knowing that \tpz\ was optimized for \pzpdf\ ensemble metrics rather than the stacked estimator of the redshift distribution.
A possible explanation is the choice of smoothing parameter chosen during validation, which affects \pzpdf\ widths as well as overall redshift bias and could be modified to improve performance under the \pzpdf\ metrics.

The first three moments of the stacked $\hat{N}(z)$ distribution relative to the empirical estimate of the truth distribution are given in Table~\ref{tab:moments}.
Accuracy of the moments varies widely between codes, raising concerns about the propagation to cosmological analyses.
%\red{[mention that we are calculating moments for the entire sample, will look at fiducial tomographic bins in a follow-up paper (unless group and reviewers realy think we should include in this paper.]}
%\red{FlexZBoost has some of the best $n(z)$ statistics, but the moments are not good.  Why is this?  Is it the over/underprediction of the high redshift part of the distribution?  Discuss this after talking with Rafael.}

\begin{table}
\setlength{\tabcolsep}{2pt}
\caption{Moments of the stacked estimator $\hat{N}(z)$ of the redshift distribution.
Most of the codes considered recover the moments of $\tilde{N}(z)$}
\label{tab:moments}
\begin{tabular}{lccc}
\hline
\hline
 \multicolumn{4}{l}{Moments of $\hat{N}(z)$} \\
\hline
Estimator  & mean       & variance    & skewness \\
Empirical ``truth'' & 0.701 & 0.630 & 0.671  \\
\hline
\annz       & 0.702      & 0.625      & 0.653    \\
\bpz        & 0.699      & 0.629      & 0.671    \\
\delight    & 0.692      & 0.609      & 0.638    \\
\eazy       & 0.681      & 0.595      & 0.619    \\
\flexzboost & 0.694      & 0.610      & 0.631    \\
\gpz        & 0.696      & 0.615      & 0.639    \\
\lephare    & 0.718      & 0.668      & 0.741    \\
\metaphor   & 0.705      & 0.628      & 0.657    \\
\cmnn       & 0.701      & 0.628      & 0.667    \\
\skynet     & 0.743      & 0.708      & 0.797    \\
\tpz        & 0.700      & 0.619      & 0.643    \\
\hline
\trainz	    & 0.699 		 & 0.627 	    & 0.666 \\
\end{tabular}
\end{table}

\skynet\ exhibits redshift bias in Figure~\ref{fig:nz} and is a clear outlier in the first moment of $\hat{N}(z)$ in Table~\ref{tab:moments}.
The \skynet\ algorithm employs a random subsampling of the training set without testing that the subset is representative of the full population, and the implementation used here does not upweight rarer low- and high-redshift galaxies, as in \citet{Bonnett:15}, suggesting a possible cause that may be addressed in future work.

\subsection{Performance on individual \pzpdf s}
\label{sec:cdelossresults}

The values of the CDE loss statistic of individual \pzpdf\ accuracy are provided in Table~\ref{tab:cdeloss}.
It is worth noting that strong performance on the CDE loss should imply strong performance on the other metrics, though the inverse is not necessarily true.
Thus the CDE loss is the most effective metric for generic science cases.

\begin{table}  %%% DATA TABLE %%%
\centering
\caption{CDE loss statistic of the individual \pzpdf s for each code.
A lower value of the CDE loss indicates more accurate individual \pzpdf s, with \cmnn\ and \flexzboost\ performing best under this metric.}
\label{tab:cdeloss}
\begin{tabular}{lr}
\hline
Photo-$z$ Code & CDE Loss \\
\hline
\annz 	    & $-6.88$ \\
\bpz 		    & $-7.82$ \\
%\textsc{Delight} 	& $-4.06$ \\
\delight    & $-8.33$\\
%\textsc{EAZY} 		& $-7.97$ \\
\eazy       & $-7.07$ \\
%\textsc{FlexZBoost} & $-11.51$ \\
\flexzboost & $-10.60$\\
\gpz		    & $-9.93$ \\
\lephare 	  & $-1.66$ \\
\metaphor 	& $-6.28$ \\
%\textsc{CMNN} 		& $-$ \\
\cmnn       & $-10.43$ \\
\skynet 	  & $-7.89$ \\
\tpz 		    & $-9.55$ \\
\hline
\trainz		  & $-0.83$ \\
%\textsc{Frankenz}	& $-$  \\
%\hline
\end{tabular}
\end{table}

This metric is the only one that can appropriately penalize \trainz\ and indicates strong performance for \cmnn\ and \flexzboost, the latter of which is optimized for this metric.
%Empirically, we have found that PIT RMSE is not as closely correlated to CDE loss as it is to the $N(z)$ statistics.

%\subsection{Response of Individual Codes}
%\label{sec:res:pz_indiv_codes}
%\red{this may be incorporated in to 5.1 and 5.2 rather than live in its own section!}


%\red{HOW CODES deal with negative fluxes, and magnitude uncertainties}

%\red{overall conclusions of the Results section}

\subsection{Interpretation of metrics}
\label{sec:caution}
%Caution on metrics and photo-$z$ codes}\label{sec:caution}
%(Alex Malz, Sam Schmidt)

We remind the reader that contributed codes were given a goal of obtaining accurate \pzpdf s, not an accurate stacked estimator of the redshift distribution, so we do not expect the same codes to necessarily perform well for both classes of metrics.
Furthermore, our metrics are not necessarily able to assess the fidelity of individual \pzpdf s relative to true posteriors.
Metric-specific performance implies that we may need multiple \pzpdf\ approaches tuned to each metric in order to maximize returns over all science cases in large upcoming surveys.

The \trainz\ estimator, which assigns every galaxy a \pzpdf\ equal to $N(z)$ of the training set as described in Section~\ref{sec:trainz}, is introduced as a control case to demonstrate this point via \textit{reductio ad absurdum}.
Because our training set is perfectly representative of the test set, $N(z)$ should be identical for both sets down to statistical noise.
\textit{We make the alarming observation that \trainz\ outperforms all codes on the PIT-based metrics, and all but one code on the $N(z)$ based statistics.}
%\textit{Explain the connection between \trainz\ and these metrics.}

The CDE loss and point estimate metrics appropriately penalize \trainz's naivete.
As shown in Appendix~\ref{sec:pointmetrics}, \trainz ~has identical $ZPEAK$ and $ZWEIGHT$ values for every galaxy, and thus the \pz\ point estimats are constant as a function of true redshift, i.e. a horizontal line at the mode and mean of the training set distribution respectively.
The explicit dependence on the individual posteriors in the calculation of the CDE loss, described in Section~\ref{sec:cdelossresults}, distinguishes this metric from those of the \pzpdf\ ensemble and stacked estimator of the redshift distribution, despite their prevalence in the \pz\ literature.

% While looking at one, or even most of our metrics would have given the impression that this estimator was nearly optimal, red flags in CDE loss and point estimates reveal the problems.
%[more caution on over-reliance on metrics, and making sure that metrics test all of what you want] \red{this definitely still needs work}
In summary, context is crucial to defending against deceptively strong performers such as \trainz; the best \pzpdf\ method is the one that most effectively achieves our science goals, not the one that performs best on a metric that does not reflect those goals.
In the absence of clear goals or the information necessary for a principled metric definition, we must think carefully before choosing a single metric.
