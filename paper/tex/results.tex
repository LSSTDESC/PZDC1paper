\section{Results}
\label{sec:results}
%(Sam Schmidt, Bryce Kalmbach, Johann Cohen Tanugi, Rongpu Zhou, Alex Malz)

We begin with a demonstrative visual inspection of the \pzpdf s produced by each code for individual galaxies.
Fig.~\ref{fig:pz_examples} shows the \pzpdf s for four galaxies chosen as examples of \pzpdf\ archetypes: a narrow unimodal redshift solution, a broad unimodal solution, a bimodal distribution, and a multimodal distribution.

\begin{figure*}
%\centering
\includegraphics[width=0.49\textwidth]{fig/pz_12codes_261931_noseaborn_crop.jpg}\includegraphics[width=0.49\textwidth]{fig/pz_12codes_471167_noseaborn_crop.jpg}\\
\includegraphics[width=0.49\textwidth]{fig/pz_12codes_713178_noseaborn_crop.jpg}\includegraphics[width=0.49\textwidth]{fig/pz_12codes_982747_noseaborn_crop.jpg}
\caption{The individual \pzpdf s (blue) distributions produced by the twelve codes (small panels) on four exemplary galaxies' photometry (large panels) with different true redshifts (red).
The \pzpdf s of all codes share some features for the example galaxies due to physical color degeneracies and photometric errors: tight unimodal $p(z)$ (upper left), broad unimodal $p(z)$ (upper right), bimodal $p(z)$ (lower right), and complex/multimodal $p(z)$ (lower left).
The diverse algorithms and implementations induce differences in small-scale structure and sensitivity to physical systematics.}
\label{fig:pz_examples}
\end{figure*}

The most striking differences between codes are due to small-scale features induced by the interaction between the shared piecewise constant parameterization of $200$ bins $0 < z < 2$ of Section~\ref{sec:metrics} and the smoothing conditions or lack thereof in each algorithm.
The $\rm{d}z = 0.01$ redshift resolution is sufficient to capture the broad peaks of faint galaxies' \pzpdf s with large photometric errors but is too broad to resolve the narrow peaks for bright galaxies' \pzpdf s with small photometric errors.
This observation is consistent with the findings of \citet[]{Malz:qp} that the piecewise constant parameterization underperforms in the presence of small-scale structures.
However, the shared small-scale features of \annz, \metaphor, \cmnn, and \skynet\ are a result of various weighted sums of the limited number of training set galaxies with colors similar to those of the test set galaxy in question, with behavior closer to classification than regression in the case of \annz.
The settings used on \gpz\ in this work forced broadening of the single Gaussian to cover the multimodal redshift solutions of the other codes.
%Interestingly, while \textsc{ANNz2} shows an abundance of small scale structure in individual $p(z)$ measurements (see Fig.~\ref{fig:pz_examples}), the summed $\hat{N}(z)$ is rather smooth, where the small scale features average out.  This is not the case for the two other codes that show an abundance of substructure in their individual $p(z)$: both \textsc{CMNN} and \textsc{SkyNet} show small scale features both in $p(z)$ and $\hat{N}(z)$.
%In contrast, FlexZBoost, for example, can return estimates on any grid without compression errors as itâ€™s a basis expansion method where only the expansion coefficients need to be stored.
%Codes with a native output format other than the shared piecewise constant binning scheme (or one that can be losslessly converted to it) may suffer from loss of information when converting to it, which could artificially favor some codes over others in a limited number of cases, for example bright galaxies with very narrow $p(z)$ where the true peak falls between grid points.  We will discuss PDF storage in Section~\ref{sec:discussion}.
%Furthermore, the fidelity of photo-$z$ interim posteriors in this format varies with the quality of the photometry.
%Switching to a quantile based parameterization may be more costly computationally, for example template-based codes would need to test more grid point in order to resolve the quantiles for bright galaxies.  However, the computational time for template based codes scales roughly linearly with the number of grid points, so this may be a reasonable thing to implement.
% We will discuss this further in Section~\ref{sec:discussion}.
% \red{someone review this statement to make sure that I'm saying this correctly!}

\subsection{\Pzpdf\ ensembles}
\label{sec:pitqq}

Fig.~\ref{fig:pitqq} shows a histogram of PIT values, QQ plot, and QQ difference plot relative to the ideal diagonal, showcasing the biases and trends in the average accuracy of the \pzpdf s for each code.
The high QQ values (more high than low PIT values) of \bpz, \cmnn, \delight, \eazy, and \gpz\ indicate \pzpdf s biased low, and the low QQ values (more low than high PIT values) of \skynet\ and \tpz\ indicate \pzpdf s biased high.

\begin{figure*}
\centering
\includegraphics[width=0.74\textwidth]{fig/PITANDQQplot_12codes_crop.jpg}
\caption{The QQ plot (red) and PIT histogram (blue) of the \pzpdf\ codes (panels) along with the ideal QQ (black dashed diagonal) and ideal PIT (gray horizontal) curves, as well as a difference plot for the QQ difference from the ideal diagonal (lower inset).
The twelve codes exhibit varying degrees of four deviations from perfection: an overabundance of PIT values at the centre of the distribution indicate a catalogue of overly broad \pzpdf s, an excess of PIT values at the extrema indicates a catalogue of overly narrow \pzpdf s, catastrophic outliers manifest as overabundances at PIT values of 0 and 1, and asymmetry indicates systematic bias, a form of model misspecification.}
\label{fig:pitqq}
\end{figure*}

The PIT histograms of \delight, \cmnn, \skynet, and \tpz\ feature an underrepresentation of extreme values, indicative of overly broad \pzpdf s, while the overrepresentation of extreme values for for \metaphor indicate overly narrow \pzpdf s.
These five codes in particular have a free parameter for bandwidth, which may be responsible for this vulnerability, in spite of the opportunity for fine-tuning with perfect prior information.
\flexzboost's ``sharpening'' parameter (described in Section~\ref{sec:flexzboost}) played a key role in diagonalizing the QQ plot, indicating a common avenue for improvement in the approaches that share this type of parameter.
On the other hand, the three purely template-based codes, \bpz, \eazy, and \lephare, do not exhibit much systematic broadening or narrowing, which may indicate that complete template coverage effectively defends from these effects.

\begin{table}
\setlength{\tabcolsep}{2pt}
\centering
\caption{The catastrophic outlier rate as defined by extreme PIT values.
We expect a value of 0.0002 for a proper Uniform distribution.
An excess over this small value indicates true redshifts that fall outside the non-zero support of the $p(z)$.}
\label{tab:pitoutlier}
\begin{tabular}{lc}
\hline
Photo-z Code & fraction PIT$<10^{-4}$ or $>$0.9999\\
\hline
\annz       & 0.0265\\
\bpz        & 0.0192\\
\delight    & 0.0006\\
\eazy       & 0.0154\\
\flexzboost & 0.0202\\
\gpz        & 0.0058\\
\lephare    & 0.0486\\
\metaphor   & 0.0229\\
\cmnn       & 0.0034\\
\skynet     & 0.0001\\
\tpz        & 0.0130\\
\hline
\trainz     & 0.0002\\
\end{tabular}
\end{table}

Though the spikes in the first and last bin of the PIT histogram were cut off in Figure~\ref{fig:pitqq} for visualization, the catastrophic outlier rates are provided in Table~\ref{tab:pitoutlier}.
As expected, \trainz\ achieves precisely the 0.0002 value expected of an ideal PIT distribution.
\annz, \flexzboost, \lephare, and \metaphor\ have notably high catastrophic outlier rates $> 0.02$, exceeding 100 times the ideal PIT rate, meriting further investigation.

\begin{figure*}
\centering
\includegraphics[width=0.74\textwidth]{fig/KSvsCvMvsAD_PIT_withnull_jpg.jpg}
\caption{A visualization of the Kolmogorov-Smirnoff (KS, blue diamond), Cramer-von Mises (CvM, black star), and Anderson-Darling (AD, red asterisk) statistics for the PIT distributions.
There is generally good agreement between these statistics, with differences corresponding to the codes with outstanding catastrophic outlier rates, a reflection in the differences in how each statistic weights the tails of the distribution.}
\label{fig:pit_stats}
\end{figure*}

Fig.~\ref{fig:pit_stats} displays the values of the KS, CvM, and AD test statistics between the PIT distribution and a uniform distribution $U(0, 1)$, highlighting the relative rather than absolute numbers.
%\red{Can p-values be supplied for each statistic? The statistics themselves are difficult to interpret, other than ``lower is better'' (p-value in skgof was broken, having trouble finding 1-sample KS calculation for uniform distribution)}
\metaphor\ and \lephare\ perform well under the AD but poorly under the KS and CvM due to their high catastrophic outlier rates.
\annz\ and \flexzboost\  are the top scorers under these metrics of the PIT distribution.
\annz's strong performance can be attributed to an aspect of the training process in which training set galaxies with a PIT that more closely matches the percentiles of the DC1 training set's redshift distribution are upweighted; in effect, these quantile-based metrics were part of the algorithm itself that may or may not serve it well under more realistic experimental conditions.

\subsection{Metrics of the stacked estimator of the redshift distribution}
\label{sec:stackedmetrics}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig/NZsumplot_12codes_scottsrule.jpg}
\caption{The stacked $p(z)$ produced by each photo-$z$ code ($\hat{N}(z)$, red) compared to the spectroscopic redshift distribution ($N'(z)$, blue).
Varying levels of agreement are seen in the codes.
Both $\hat{N}(z)$ and $N'(z)$ in all codes are smoothed using a single bandwidth chosen via Scott's rule.}
\label{fig:nz}
%\aim{Arguably this could also be clarified by showing the true $n'(z)$ once and then differences from it for each cod's stacked estimator\dots}\scc{what if we plot the difference between true and stacked rather than soverposing the two lines? Sam please do not hate me.}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig/KSvsCvMvsAD_NZ_withnull_jpg.jpg}
\caption{A visual representation of the Kolmogorov-Smirnoff (KS, blue diamond), Cramer-von Mises (CvM, black star), and Anderson-Darling (AD, red asterisk) statistics for the $\hat{N}(z)$ distributions.
The statistics are correlated, the codes with the lowest KS statistics tend to have the lowest CvM and AD statistics.
\textsc{CMNN} performs markedly better than the others in reconstructing the overall $N(z)$ distribution, while \textsc{SkyNet} scores poorly due to an overall bias in its redshift predictions.}
\label{fig:nz_stats}
\end{figure*}

Fig.~\ref{fig:nz} shows the stacked $\hat{N}(z)$ distribution compared to the true redshift distribution $N'(z)$ for all tested codes.
The red line indicates the summed $p(z)$ for each code, while the blue line shows the true redshift distribution.
All distributions are smoothed via kernel density estimation (KDE) with a common bandwidth chosen via Scott's rule \citep{Scott:1992} in order to minimize differences in small-scale features and make for a more uniform comparison between codes.
While Scott's rule is used to display $N'(z)$ in the figure, all quantitative statistics are computed via the empirical CDF, and are thus unaffected by bandwidth/smoothing choice.
%New text for the new figure
As expected, \textsc{TrainZ} is in excellent agreement with the true redshift distribution: as the training sample is selected from the same underlying distribution as the test set, the redshift distributions are identical, up to Poisson fluctuations due to the finite number of sample galaxies.
\textsc{CMNN} is also in excellent agreement for similar reasons: with a representative training sample of galaxies spanning the colour-space, the sum of the colour-matched neighbour redshifts should return the true redshift distribution. \textsc{FlexZBoost} and \textsc{TPZ} also show very good agreement, with only slight departures, with an over/under-prediction in the high redshift tail of $\hat{N}(z)$ evident around $z\sim1.4$.
In fact, several of the other codes show an excess at $z \sim 1.4$, particularly the template-based codes \textsc{BPZ}, \textsc{EAZY}, and \textsc{LePhare}.
This is likely due to the $4000\ {\rm \AA}$ break passing through the gap between the $z$ and $y$ filters, resulting in a drastic change in $z-y$ colour for galaxies in this redshift range.
With a relative dearth of strong features blue-ward of the $4000\ {\rm \AA}$ break in most galaxy SEDs, the colour change in the two reddest filter bandpasses of a survey has a large influence on the redshift determination.
The $z\sim1.4$ feature is one of the most prominent sources of larger uncertainty in individual galaxy $p(z)$.
In our sample individual galaxy $p(z)$'s tend to be broader around $z\sim1.4$ and point estimates are more uncertain in this regime, as is readily seen in the point-estimate plots shown in Fig.~\ref{fig:pz_pointestimates} and described in the Appendix.
This feature is not unique to this dataset, it is a common occurrence in photo-$z$ estimation.
The fact that similar excesses appear in Figure~\ref{fig:nz} for \textsc{ANNz2} and \textsc{METAPhoR} shows that the effect is not limited to template-based codes.
However, the lack of such a feature in the other codes shows that it is possible to eliminate the degeneracies.
Further study on this issue may provide a solution for codes that suffer from this shortcoming.

%end of new text (and slightly modified text)
Two of the machine learning based codes, \textsc{ANNz2} and \textsc{METAPhoR}, appear to be over-trained, adding excess galaxy probability to the redshift peaks with the largest number of training galaxies, and missing probability in the troughs where training galaxies are of fewer number.
Given that our training data is drawn from the same galaxy population as the test set, and our data has prominent peaks in $N'(z)$, perhaps it is not unexpected that such overtraining occurs in some codes, though the fact that it does not occur in all training-based codes indicates that it may be due to specifics of the implementations and bandwidth choices of \textsc{ANNz2} and \textsc{METAPhoR}.
This, once again, emphasizes that care much be taken in choice of bandwith parameters in individual codes in order to avoid such overtraining.
%\aim{Move the text on Scott's Rule to Discussion?}
%Scott's rule \citep{Scott:1992} was used to determine the smoothing bandwidth of the true redshift sample: that is, a single smoothing scale was chosen to represent the true redshift sample for all twelve codes.
%As with the $p(z)$ values in Figure~\ref{fig:pitqq}, different levels of substructure are obvious for the different codes.
%While Scott's rule provides a relatively good general smoothing scale to represent the true $N'(z)$, there are smaller scale fluctuations: while \textsc{FlexZBoost} and \textsc{CMNN} appear somewhat discrepant in Fig.~\ref{fig:nz}, they are actually the two most accurate in terms of their quantitative measurements.
%\textsc{SkyNet} also shows excess small-scale structure, but also shows significant overestimate of the high redshift galaxy population.
\textsc{SkyNet} shows an obvious redshift bias, evident both visually in Figure~\ref{fig:nz} and in the first moment of N(z) listed in Table~\ref{tab:moments}, where it is clearly an outlier.
\textsc{SkyNet} employed a method where a random sample of training galaxies was chosen, but there was no test that the subset was completely representative of the overall redshift distribution.
Unlike the previous implementation of \textsc{SkyNet} in \citet{Bonnett:15}, no effort was made to add extra weight to more rare low and high redshift galaxies.
Either of these decisions could be the cause of the bias seen in our results.
Future runs of \textsc{SkyNet} will explore these implementation choices and their effects.

Figure~\ref{fig:nz_stats} shows the quantitative Kolmogorov-Smirnoff (KS), Cramer-Von Mises (CvM), and Anderson Darling (AD) test statistics for each of the codes for the $\hat{N}(z)$ based measures.
%\red{Can p-values be supplied for each statistic? The statistics themselves are difficult to interpret, other than ``lower is better'' (no, p-values are very difficult to compute for non-uniform distributions)}
\textsc{FlexZBoost}, \textsc{CMNN}, and \textsc{TPZ} outperform the other codes in the $\hat{N}(z)$ metrics.
It is unsurprising that \textsc{CMNN} scores well, as with a near perfectly representative training set means that choosing neighbouring points in color/magnitude space should lead to excellent agreement in the final $\hat{N}(z)$ estimate.
\textsc{TPZ} performed quite poorly in $p(z)$ statistics, but results in a good fit to the overall $N(z)$.
This is somewhat surprising, as performance was optimized for accurate $p(z)$, not $\hat{N}(z)$.
During the validation stage for \textsc{TPZ}, there was a trade off between the width of the $p(z)$ when adjusting a smoothing parameter and overall redshift bias.
The optimal result in the PIT metrics, as illustrated in the shape of the QQ plot, does contain some level of bias as well as a slight underprediction of mean $p(z)$ width, which translates to poor metric scores.
This is something that will be looked into for \textsc{TPZ} in the future.

%\red{discussion of why NN and FZBoost are doing well, how we can try to improve the other codes}

Table~\ref{tab:cdeloss} shows the CDE loss statistic for each photo-$z$ code.
Once again \textsc{FlexZBoost} and \textsc{CMNN} score very well for the stacked $\hat{N}(z)$ metrics, as do \textsc{GPz} and \textsc{TPZ}.
The CDE loss measures how well individual PDFs are estimated, and codes with a low CDE loss tend to have good $\hat{N}(z)$ estimates (though the reverse is not necessarily true).
\textsc{FlexZBoost} is optimized to minimize CDE loss which may explain why the method has good ensemble metrics as well.
Note from Table~\ref{tab:cdeloss} that both \textsc{FlexZBoost} and \textsc{CMNN} have low CDE losses.  %Empirically, we have found that PIT RMSE is not as closely correlated to CDE loss as it is to the $N(z)$ statistics.
As CDE loss is a better measure of individual redshift performance, rather than ensemble distribution performance, this statistic is a better indicator of which codes will be most likely to perform well for science cases where single objects are employed.

%Table~\ref{tab:rmse} gives the root-mean-square-error (RMSE) statistics for both the PIT and N(z) estimators.  The PIT value calculates the RMSE between the quantiles shown in the QQ plot in Figure~\ref{fig:pitqq} and the diagonal, while the N(z) calculates the RMSE between the cumulative distribution of the stacked $\hat{N}(z)$ and the true redshift distribution $N'(z)$.  %\red{more about RMSE?  Or remove it? It's barely described in Section~\ref{sec:metrics} and wasn't even referred to in the text until I added this on June 29th--SJS.}

Table~\ref{tab:moments} lists the first three moments of the stacked $\hat{N}(z)$ distribution, including the moments of the ``truth'' distribution for comparison.
Several codes are able to reproduce the mean and variance of the distribution to less than a per cent, while several codes do not, which may be a cause for concern, given that mean and variance of the redshift distribution are key properties in cosmological analyses.
We note that this stated goal of the study as defined for participants was to accurately reproduce $p(z)$, the ``stacking'' of the probability distributions to estimate $\hat{N}(z)$ was not the focus as stated to the participants.
This explains why some of the best-performing empirical codes in terms of $p(z)$ measures (e.~g.~\textsc{FlexZBoost}) do not do as well at reproducing $\hat{N}(z)$ moments.
Had we defined a different parameter to optimize, in this case overall accuracy of $\hat{N}(z)$ rather than individual $p(z)$, would result in improved performance in a particular metric.
That is, optimizing photo-$z$ performance for one metric does not automatically give optimal performance for other metrics.
As previously stated, there are a variety of scientific use cases for photo-$z$'s in large upcoming surveys, and care must be taken in how the metrics used to optimize catalog photometric redshifts are defined as well as in how they are used.
This implies that we may need multiple photo-z estimators, tuned to the particular metric, in order to maximize returns over all science cases in upcoming surveys.
In addition, very few scientific use cases will employ the overall $\hat{N}(z)$ with no cuts, as we explore in this paper.
We discuss more realistic tomographic bin selections that will be explored in a follow-up paper in Section~\ref{sec:futurework}.
%\red{[mention that we are calculating moments for the entire sample, will look at fiducial tomographic bins in a follow-up paper (unless group and reviewers realy think we should include in this paper.]}
%\red{FlexZBoost has some of the best $n(z)$ statistics, but the moments are not good.  Why is this?  Is it the over/underprediction of the high redshift part of the distribution?  Discuss this after talking with Rafael.}

%\subsection{Response of Individual Codes}
%\label{sec:res:pz_indiv_codes}
%\red{this may be incorporated in to 5.1 and 5.2 rather than live in its own section!}


%\red{HOW CODES deal with negative fluxes, and magnitude uncertainties}

%\red{overall conclusions of the Results section}

\subsection{Interpretation of metrics}
\label{sec:caution}
%Caution on metrics and photo-$z$ codes}\label{sec:caution}
%(Alex Malz, Sam Schmidt)

Samples from accurate photo-$z$ posteriors should reproduce the space of $p(z,{\rm data})$.
However, it is difficult to test this reconstruction given our data set, as the galaxy distributions arise from mock objects pasted on to an underlying dark matter halo catalogue with properties designed to match empirical relations, rather than being drawn from statistical distributions in redshift.
In previous sections we have mentioned that optimizing for a specific metric does not guarantee good performance on other metrics, nor is there any guarantee that good performance by our metrics corresponds to \textit{accurate} photo-$z$ posteriors, in the sense of predicting redshifts for individual galaxies.
%\aim{Explain that samples from accurate photo-$z$ posteriors should reconstruct the space of $p(z, data)$, a test we can't check with our datasets.}
In other words, we can construct photo-$z$ estimators that provide good coverage in many of our tests, but which have very little predictive power.


The \trainz\ estimator, which assigns every galaxy a $p(z)$ equal to $N(z)$ of the training set as described in Section~\ref{sec:trainz}, is introduced as a ``null test'' to demonstrate this point via \textit{reductio ad absurdum}.
\trainz\ outperforms all codes on the PIT-based metrics, and all but one code on the $N(z)$ based statistics.
Because our training set is perfectly representative of the test set, $N(z)$ should be identical for both sets down to statistical noise.
%\textit{Explain the connection between \trainz\ and these metrics.}

The CDE loss and point estimate metrics, however, successfully identify problems with \trainz.
As shown in Appendix~\ref{sec:pointmetrics}, \trainz ~has identical $ZPEAK$ and $ZWEIGHT$ values for every galaxy, and thus the photo-$z$s are constant as a function of spec-$z$s, i.e. a horizontal line at the mode and mean of the training set distribution respectively.
The explicit dependence on the {\it individual} posteriors in the calculation of the CDE loss, described in Section~\ref{sec:CDE_loss}, distinguishes this metric from the other $p(z)$ metrics that test the overall ensemble of $p(z)$ distributions.
With a representative training set, \trainz\ will score well on the ensemble metrics, but fails miserably for metrics tied to individual redshifts presented in this paper.
We note that many of the ensemble-based metrics are prominent in the photo-$z$ literature despite their inability to identify problems such as those exemplified by \trainz.

%\aim{Explain why CDE loss is different from other metrics (isn't fooled by \trainz) and note that the others have more of a presence in the literature despite their flaw of failing to identify this severe problem.}

% While looking at one, or even most of our metrics would have given the impression that this estimator was nearly optimal, red flags in CDE loss and point estimates reveal the problems.
%[more caution on over-reliance on metrics, and making sure that metrics test all of what you want] \red{this definitely still needs work}
In summary, context is crucial to interpreting metrics and defending against deceptively well performing methods such as \trainz.
The best photo-$z$ method is the one that most effectively achieves our science goals, not the one that performs best on a metric that does not accurately reflect those goals.
In the absence of clear goals or the information necessary for a principled metric definition, we must think carefully before choosing a single metric.

%\begin{table*}  %%% DATA TABLE %%%
%\caption{KS, CvM and AD statistics for each photo-$z$ code.} \label{tab:results}
%PIT statistic
%
%\begin{tabular}{lrrrr}
%\hline
%\bf Photo-$z$ Code & \bf KS & \bf CvM & \bf AD & \bf AD \\
%                   &        &         & $v=[0.05,0.95]$ & $v=[0.01,0.99]$        \\
%\hline
%\textsc{ANNz2} 		& $0.0200$ &   $52.25$ &   $583.0$ &   $759.2$ \\
%\textsc{BPZ} 		& $0.0388$ &  $280.79$ &  $1361.4$ &  $1557.5$ \\
%%\textsc{Delight} 	& $0.1380$ & $3040.59$ & $13880.2$ & $17761.4$ \\
%\textsc{Delight}    & $0.0876$ & $1075.17$ & $4407.8$  & $6167.5$\\
%%\textsc{EAZY} 		& $0.0690$ &  $994.19$ &  $3479.5$ &  $4158.8$ \\
%\textsc{EAZY}       & $0.0723$ & $1105.58$ & $3475.0$  & $4418.6$\\
%%\textsc{FlexZBoost} & $0.0260$ &   $70.58$ &   $595.0$ &   $533.0$ \\
%\textsc{FlexZBoost} & $0.0240$ &   $68.83$ &   $567.9$ &   $478.8$\\
%\textsc{GPz} 		& $0.0449$ &  $258.56$ &   $904.8$ &  $1414.8$ \\
%\textsc{LePhare} 	& $0.0663$ &  $473.05$ &    $85.9$ &   $383.8$ \\
%\textsc{METAPhoR} 	& $0.0438$ &  $298.56$ &   $248.6$ &   $715.5$ \\
%%\textsc{CMNN} 		& $-$ & $-$ & $-$ & $-$ \\
%\textsc{CMNN}         & $0.0795$  &$1011.11$ &  $3908.2$ &  $6307.5$ \\
%\textsc{SkyNet} 	& $0.0747$ &  $763.00$ &  $2740.2$ &  $4216.4$ \\
%\textsc{TPZ} 		& $0.1138$ & $1801.74$ &  $9046.2$ & $10565.7$ \\
%%\textsc{Frankenz}	& $-$ & $-$ & $-$ & $-$ \\
%\hline
%\end{tabular}

%Stacked $n(z)$ statistic
%
%\begin{tabular}{lrrrr}
%\hline
%\bf Photo-$z$ Code & \bf KS & \bf CvM & \bf AD & \bf AD \\
%                   &        &         & $z=[0.002,2.0]$ & $z=[0.0,2.0]$        \\
%\hline
%\textsc{ANNz2} 		& $0.0237$ &  $25.676$ &   $276.4$ &   $276.0$ \\
%\textsc{BPZ} 		& $0.0131$ &  $17.587$ &   $169.0$ &   $170.9$ \\
%%\textsc{Delight} 	& $0.0184$ &  $18.975$ &   $151.5$ &   $151.5$ \\
%\textsc{Delight}    & $0.0256$ &  $42.341$ &   $253.3$ &   $253.2$ \\
%%\textsc{EAZY} 		& $0.0434$ & $167.329$ &   $811.3$ &   $817.0$ \\
%\textsc{EAZY}       & $0.0441$ & $169.977$ &   $827.7$ &   $833.3$ \\
%%\textsc{FlexZBoost} & $0.0065$ &   $2.764$ &    $24.7$ &    $24.6$ \\
%\textsc{FlexZBoost} & $0.0128$ &   $7.007$ &  $127.02$ &   $126.9$\\
%\textsc{GPz} 		& $0.0168$ &  $25.605$ &   $307.1$ &   $307.1$ \\
%\textsc{LePhare} 	& $0.0254$ &  $58.900$ &   $477.4$ &   $472.0$ \\
%\textsc{METAPhoR} 	& $0.0350$ &  $60.790$ &   $671.9$ &   $672.3$ \\
%%\textsc{CMNN} 		& $-$ & $-$ & $-$ & $-$ \\
%\textsc{CMNN}         & $0.0043$ & $0.538$   &   $5.6$   &      $5.8$ \\
%\textsc{SkyNet} 	& $0.0483$ & $385.830$ &  $2119.4$ &  $2110.3$ \\
%\textsc{TPZ} 		& $0.0126$ &   $9.483$ &   $110.7$ &   $110.6$ \\
%%\textsc{Frankenz}	& $-$ & $-$ & $-$ & $-$ \\
%\hline
%\end{tabular}
%\end{table*}

\begin{table}  %%% DATA TABLE %%%
\centering
\caption{CDE loss statistic for each photo-$z$ code.} \label{tab:cdeloss}
\begin{tabular}{lr}
\hline
\bf Photo-$z$ Code & \bf CDE Loss \\
\hline
\textsc{ANNz2} 		& $-6.88$ \\
\textsc{BPZ} 		& $-7.82$ \\
%\textsc{Delight} 	& $-4.06$ \\
\textsc{Delight}    & $-8.33$\\
%\textsc{EAZY} 		& $-7.97$ \\
\textsc{EAZY}       & $-7.07$ \\
%\textsc{FlexZBoost} & $-11.51$ \\
\textsc{FlexZBoost} & $-10.60$\\
\textsc{GPz} 		& $-9.93$ \\
\textsc{LePhare} 	& $-1.66$ \\
\textsc{METAPhoR} 	& $-6.28$ \\
%\textsc{CMNN} 		& $-$ \\
\textsc{CMNN}         & $-10.43$ \\
\textsc{SkyNet} 	& $-7.89$ \\
\textsc{TPZ} 		& $-9.55$ \\
\hline
\trainz		& $-0.83$ \\
%\textsc{Frankenz}	& $-$  \\
%\hline
\end{tabular}
\end{table}

%\begin{table}
%\setlength{\tabcolsep}{2pt}
%\begin{center}
%\caption{Root-Mean-Square-Error (RMSE) statistics for the twelve photo-z codes for both PIT and $\hat{N}(z)$ distributions.}\label{tab:rmse}
%\begin{tabular}{lcc}
%\hline
%\hline
%Root-Mean-Square-Error \\
%(RMSE) statistics \\
%\hline
%Photo-z Code & PIT RMSE & N(z) RMSE\\
%\hline
%\textsc{ANNz2}      & 0.019 & 0.0054\\
%\textsc{BPZ}        & 0.032 & 0.0050\\
%\textsc{Delight}    & 0.111 & 0.0056\\
%\textsc{EAZY}       & 0.054 & 0.0102\\
%\textsc{FlexZBoost} & 0.021 & 0.0022\\
%\textsc{GPz}        & 0.027 & 0.0042\\
%\textsc{LePhare}    & 0.028 & 0.0062\\
%\textsc{METAPhoR}   & 0.064 & 0.0081\\
%\textsc{CMNN}         & 0.108 & 0.0009\\
%\textsc{Skynet}     & 0.054 & 0.0144\\
%\textsc{TPZ}        & 0.082 & 0.0031\\
%\hline
%\trainz		& 0.0025 & 0.0013\\
%\end{tabular}
%\end{center}
%\end{table}

%\begin{table}
%\begin{center}
%caption{RMSE Statistics for the 11 Photo-z codes for DC1 $i<25.3$ ``gold'' sample}\label{tab:pitrmse}
%%\begin{tabular}{|l|c|c|c|c|}
%\begin{tabular}{lc}
%\hline
%\hline
% PIT RMSE statistics \\
%\hline
%Photo-z Code & PIT RMSE\\
%\hline
%\texttt{ANNz2}      & 0.019\\
%\texttt{BPZ}        & 0.032\\
%\texttt{Delight}    & 0.111\\
%\texttt{EAZY}       & 0.054\\
%\texttt{FlexZBoost} & 0.021\\
%\texttt{GPz}        & 0.048\\
%texttt{LePhare}    & 0.028\\
%\texttt{METAPhoR}   & 0.064\\
%\texttt{NN}         & 0.108 \\
%\texttt{Skynet}     & 0.054\\
%texttt{TPZ}        & 0.082\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}
%
%\begin{table}
%\begin{center}
%\caption{RMSE Statistics for the 11 Photo-z codes for DC1 $i<25.3$ ``gold'' sample}\label{tab:nzrmse}
%%\begin{tabular}{|l|c|c|c|c|}
%\begin{tabular}{lc}
%\hline
%\hline
% N(z) RMSE statistics \\
%\hline
%hoto-z Code & N(z) RMSE\\
%\hline
%\texttt{ANNz2}      & 0.0054\\
%\texttt{BPZ}        & 0.0050\\
%\texttt{Delight}    & 0.0056\\
%\texttt{EAZY}       & 0.0102\\
%\texttt{FlexZBoost} & 0.0022\\
%\texttt{GPz}        & 0.0042\\
%\texttt{LePhare}    & 0.0062\\
%\texttt{METAPhoR}   & 0.0081\\
%\texttt{NN}         & 0.0009\\
%\texttt{Skynet}     & 0.0144\\
%\texttt{TPZ}        & 0.0031\\
%\end{tabular}
%\end{center}
%\end{table}

\begin{table}
\setlength{\tabcolsep}{2pt}
\caption{Moments of the stacked $\hat{N}(z)$ distribution}\label{tab:moments}
\begin{tabular}{lccc}
\hline
\hline
 \multicolumn{4}{l}{Stacked $n(z)$ Moments} \\
\hline
              & 1st Moment & 2nd Moment & 3rd Moment \\
\textsc{Truth}     & 0.701      &   0.630    & 0.671  \\
\hline
Photo-z Code       & 1st Moment & 2nd Moment & 3rd Moment\\
\hline
\textsc{ANNz2}     & 0.702      & 0.625      & 0.653    \\
\textsc{BPZ}       & 0.699      & 0.629      & 0.671    \\
\textsc{Delight}   & 0.692      & 0.609      & 0.638    \\
\textsc{EAZY}      & 0.681      & 0.595      & 0.619    \\
\textsc{FlexZBoost}& 0.694      & 0.610      & 0.631    \\
\textsc{GPz}       & 0.696      & 0.615      & 0.639    \\
\textsc{LePhare}   & 0.718      & 0.668      & 0.741    \\
\textsc{METAPhoR}  & 0.705      & 0.628      & 0.657    \\
\textsc{CMNN}        & 0.701      & 0.628      & 0.667    \\
\textsc{Skynet}    & 0.743      & 0.708      & 0.797    \\
\textsc{TPZ}       & 0.700      & 0.619      & 0.643    \\
\hline
\trainz	   & 0.699 		& 0.627 	& 0.666 \\
\end{tabular}
\end{table}
