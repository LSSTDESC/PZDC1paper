\section{Methods}
\label{sec:pzcodes}

%(Sam Schmidt, Rongpu Zhou, Ibrahim Almosallam, Eric Nuss, Johann Cohen Tanugi, John Soo)

Here we outline the photo-$z$ PDF codes tested in this study. In total, eleven distinct codes are tested.  This sample is not comprehensive, codes were chosen based on the expertise available within the group; however, those chosen do cover a broad range of the current-generation codes.  Both template-based and machine learning approaches are included and each are described separately in Secs.~\ref{sec:templatecodes} and \ref{sec:trainingcodes} respectively. The list of codes are summarized in Table.~\ref{tab:list_of_codes}.  All code runners were asked to output redshift estimates on 200 linear-spaced bins between redshifts 0 and 2.

\begin{table*}  %%% DATA TABLE %%%
\caption{List of photo-$z$ codes featured in this study. ML here means machine learning.} \label{tab:list_of_codes}\resizebox{\textwidth}{!}{
\begin{tabular}{llll}
\hline
\bf Code 			& \bf Type & \bf Paper & \bf Website \\
\hline
\textsc{BPz} 		& template 			& \citet{Benitez:00}		& \url{http://www.stsci.edu/~dcoe/BPZ/} \\
\textsc{EAZY} 		& template 			& \citet{Brammer:08}		& \url{https://github.com/gbrammer/eazy-photoz} \\
\textsc{LePhare} 	& template 			& \citet{Arnouts:99}		& \url{http://www.cfht.hawaii.edu/~arnouts/lephare.html} \\
\textsc{ANNz2} 		& ML 	& \citet{Sadeh:16}			& \url{https://github.com/IftachSadeh/ANNZ} \\
\textsc{Delight} 	& ML/template 	& \citet{Leistedt:17}		& \url{https://github.com/ixkael/Delight} \\
\multirow{ 2}{*}{\textsc{FlexZBoost}} & \multirow{ 2}{*}{ML} 	& \multirow{ 2}{*}{\citet{Izbicki:17}}		& \url{https://github.com/tpospisi/flexcode};\\
					& 		&							& \url{https://github.com/rizbicki/FlexCoDE}\\
%\textsc{Frankenz} 	& ML 	& \citet{Speagle:frankenz}	& \url{https://github.com/joshspeagle/frankenz} \\
\textsc{GPz} 		& ML	& \citet{Almosallam:15b}	& \url{https://github.com/OxfordML/GPz} \\
\textsc{METAPhoR} 	& ML 	& \citet{Cavuoti:17}		& \url{http://dame.dsf.unina.it}\\
\textsc{CMNN} 		& ML 	& \citet{Graham:17}			& - \\
\textsc{SkyNet} 	& ML 	& \citet{Graff:14}			& \url{http://ccpforge.cse.rl.ac.uk/gf/project/skynet/} \\
\textsc{TPZ} 		& ML 	& \citet{Carrasco_Kind:13}	& \url{https://github.com/mgckind/MLZ} \\
\hline
\trainz 	& N/A	& See Section~\ref{sec:method:trainz} & \\
\end{tabular}}
\end{table*}

%\red{The questions that must be answered for each code are\dots}
The questions that must be answered for each code are: what unique features are included in the specific implementation that influence the output $p(z)$.  What form of validation was performed with the training data, how were photometric uncertainties employed in the analysis, how were negative fluxes treated, what specific prior form was employed (for template based codes), or what specific machine learning architecture was used (for ML codes)?
%\scc{which parameter (in a general meaning) have been used, which kind of validation has been performed (if any)}
\subsection{Template-based Approaches}
\label{sec:templatecodes}
We test three publicly available and commonly used template-based codes: \textsc{BPZ}, \textsc{EAZY}, and \textsc{LePhare}.  All three codes follow the standard procedure for template-based redshift estimation: calculate model fluxes for a set of template spectral energy distributions (SEDs) on a grid of redshift values and calculating a $\chi^2$ merit function using the observed and model fluxes:
\begin{equation} \label{eq_temp_chi}
\chi^2(z,T,A) = \sum_i^{N_{\rm filt}}\left(\frac{F^i_{\rm{obs}}âˆ’A \times F^i_{\rm{pred}}(T,z)}{\sigma^i_{\rm{obs}}}\right)^2
\end{equation}
\noindent  where $A$ is a normalization factor, $F^i_{\rm{pred}}(T,z)$ is the flux predicted for a template $T$ at redshift $z$. $F^i_{\rm{obs}}$ is the observed flux in a given band $i$ and $\sigma^i_{\rm{obs}}$ is the observed flux error. $N_{\rm tot}$ is the total number of filters, in our case the six $ugrizy$ LSST filters.  Specific implementation details of each code, e.~g.~prior form and implementation, are described below.

\subsubsection{BPZ}
\label{sec:BPZ}
%(Sam Schmidt)

\textsc{BPZ}\footnote{\url{http://www.stsci.edu/~dcoe/BPZ/}} \citep[Bayesian Photometric Redshift,][]{Benitez:00} is a template-based photo-$z$ code that compares the expected colors ($C$) calculated for a set of spectral energy distribution (SED) types/templates ($T$) to the observed colors to calculate the likelihood of observing colors at each redshift for each type, $p(C|z,T)$.  The likelihoods at each redshift are related to the $\chi^2$ in Equation~\ref{eq_temp_chi} by the simple form: likelihood $\propto \, e^{-\chi^2/2}$. The code employs an empirically determined Bayesian prior in apparent magnitude ($m_0$) and SED-type. Assuming that the SED-types are spanning and exclusive, we can determine the redshift posterior $p(z|C,m_0)$ by marginalizing over all SED-types with a simple sum \citep[Eq.~3 from][]{Benitez:00}:

\begin{equation} \label{eq:redshift_posterior}
p(z|C,m_0)\,\propto\, \sum_{T}p(z,T|m_0)\,p(C|z,T)
\end{equation}
\noindent where the first term on the right-hand side is the Bayesian prior and the second term is the traditional likelihood. The prior is assumed to have the form: $p(z,T|m_0)=p(T|m_0)\,p(z|T,m_0)$, i.e. it parameterizes the prior as an evolving type fraction with apparent magnitude, combined with a prior on the expected redshift probability distribution as a function of both apparent magnitude and SED-type.

In this paper we use \textsc{BPZ v 1.99.3}. The template set employed here is the set of $100$ discrete SEDs described in Section~\ref{sec:buzztemplates}
%, and the $129$ Brown SEDs for the Galacticus dataset.
To keep the number of free parameters to a manageable level the SEDs in the training set are sorted by the rest-frame $u$-$g$ colour and split into three ``broad'' SED classes, equivalent to the \texttt{E}, \texttt{Sp} and \texttt{Im/SB} types in \citet{Benitez:00}. We assume the same functional form for the Bayesian priors as used by \citet{Benitez:00}, and utilize the training-set galaxies with known SED-type, redshift, and apparent magnitude to determine the type fractions and the best fit for the eleven free parameters of the prior.
%For photo-$z$ point estimates we use the \texttt{Z\_B} output parameter.
For galaxies that are not detected in a measured band, the placeholder value is replaced with an estimate of the one $\sigma$ detection limit in that particular band, i.~e.~a value close to the estimated sky noise threshold.
The type-marginalized $p(z)$ is generated by setting the parameter \texttt{PROBS\_LITE=TRUE} in the \textsc{BPZ} parameter file.

\subsubsection{EAZY} \label{sec:eazy}
%(Rongpu Zhou)

\textsc{EAZY}\footnote{\url{https://github.com/gbrammer/eazy-photoz}} \citep[Easy and Accurate Photometric Redshifts from Yale,][]{Brammer:08} is a template-based photo-$z$ code that includes several features that improve on the basic $\chi^2$ fit used in many template codes. It can fit the observed photometry with SEDs created from a linear combination of a set of templates at each redshift, and the best-fit SED is found by simultaneously fitting one, two or all of the templates by minimizing $\chi^2$. The minimized $\chi^2(z)$ is then combined with an apparent magnitude prior to obtain the posterior redshift probability distribution, although some argue that this is not the mathematically correct way of calculating the posteriors. \textsc{EAZY} can also account for the uncertainties in the templates by adding an empirically derived template error in quadrature as a function of redshift to the flux errors.

In this paper we use the all-templates mode, which fits the photometric data with a linear combination of the five basis templates. We employed the $5$ basis templates described in Section~\ref{sec:buzzard}, and set the template error to zero since these same templates were used to produce the simulated catalog photometry. %\red{The quality parameter \texttt{Qz} is...}
The likelihoods include the application of a type-independent apparent magnitude prior estimated from the training data.

\subsubsection{LePhare}\label{sec:lephare}
%(C\'ecile Roucelle, Eric Nuss, Johann Cohen-Tanugi)

\textsc{LePhare}\footnote{\url{http://www.cfht.hawaii.edu/~arnouts/lephare.html}}\citep[Photometric Analysis for Redshift Estimate,][]{Arnouts:99,Ilbert:06} is a photo-$z$ reconstruction code based on a $\chi^2$ template-fitting procedure. The observed colors are matched with the colours predicted from a set of spectral energy distribution (SED) which can be either synthetic or based on a semi-empirical approach. %\textsc{LePhare} has been used to produce the COSMOS2015 photo-$z$ catalogue \citep{Laigle:16}.

Each SED is convolved with the simulated LSST filter transmission curves (accounting for instrument efficiency).  The computed photo-$z$ is then the value that minimizes the merit function $\chi^2 (z,T,A)$ from \citet[]{Arnouts:99}, and given in Equation~\ref{eq_temp_chi}.

In this paper we use \textsc{LePhare v 2.2}.
The set of templates used for fitting the photo-$z$'s are the 100 discrete Buzzard SED templates as described in section~\ref{sec:buzztemplates}, and the full $p(z)$ corresponds to the likelihoods calculated at each point on our $z$-grid.


\subsection{Training-based Codes}
\label{sec:trainingcodes}
The training-based codes use a variety of algorithms in order to estimate $p(z)$. 
Some aspects of data treatment were left to the individual code runners, for example, whether/how to split the available data with known redshifts into separate training and validation sets.  The treatment of objects not detected in one or more bands was also left to the code runners.  There are varying conventions among training-based codes, and no one prescription dominates in the photo-$z$ literature.  This and other details of the code implementations are described below.  

\subsubsection{ANNz2}
\label{sec:annz2}
%(John Soo)

\textsc{ANNz2}\footnote{\url{https://github.com/IftachSadeh/ANNZ}} \citep{Sadeh:16} is a software package that has the ability to employ several machine learning algorithms, including artificial neural networks (ANN), boosted decision tree (BDT) and k-nearest neighbour (KNN). Using the Toolkit for Multivariate Data Analysis (TMVA) with ROOT\footnote{\url{http://tmva.sourceforge.net/}}, it can run multiple machine learning algorithms for a single training and outputs photo-$z$'s based on a weighted average of their performances.

\textsc{ANNz2} is capable of producing both photo-$z$ point estimates and redshift posterior probability distributions $p(z)$.  It can also perform classifications, and supports reweighting between samples. The PDFs are produced by propagating the intrinsic uncertainty on the input parameters and the uncertainty in the machine learning method to the expected photo-$z$ solution, averaged over multiple runs weighted based on the performance of each run. \textsc{ANNz2} presents its photo-$z$ uncertainty different from many codes by using the KNN method: it estimates the photo-$z$ bias between each object and a fixed number of nearest neighbours in parameter space, it then takes the $68$th percentile width of the distribution of the bias. This is based on the implication that objects with similar photometric properties would have similar uncertainties, and therefore the photometric errors of the inputs are not propagated into the code.

In this study, \textsc{ANNz2 v. 2.0.4} was used. A set of $5$ ANNs with architecture $6:12:12:1$ ($6$ $ugrizy$ inputs, $2$ hidden layers with $12$ nodes each, and $1$ output) with different random seeds are used during each training. %Training was conducted on a subset of the given training sample which satisfies $i\le25.3$ as it would have improved the results compared to training on the entire sample.
Half of the training set is used as a validation set to prevent overtraining. All training objects are set to have detected magnitudes, however the non-detections (mag$=99$) in the testing set are replaced with the mean of that particular band.


\subsubsection{Color-Matched Nearest-Neighbours}
\label{sec:cmnn}
%(Melissa Graham)

The nearest-neighbours color-matching photometric redshift estimator (\textsc{CMNN}) is presented in \citet[][herafter G18]{Graham:17}. This method uses a training set of galaxies with known redshifts that has equivalent or better photometry as the test set in terms of quality and filter coverage. For each galaxy in the test set we identify a color-matched subset of training galaxies, choose one (e.g. the nearest-neighbour or a random selection), and use its known redshift as the photo-$z$. This color-matched subset is identified by first calculating the Mahalanobis distance $D_M$ in color-space between the test galaxy and all training-set galaxies: the difference between the test and a training set galaxy's color divided by the photometric error, summed over all colors (i.e., $u$-$g$, $g$-$r$, $r$-$i$, $i$-$z$, and $z$-$y$). Then, the threshold value for $D_M$ that define a good color match is set by the percent point function (PPF): for example, for $N_{\rm dof}=5$, PPF$=95$ per cent of all training galaxies consistent with the test galaxy will have $D_M < 11.07$ (where $N_{\rm dof}$, the number of degrees of freedom, is the number of colors). For a given test galaxy, the $p(z)$ is the normalized distribution of the true catalogue redshifts of this color-matched subset of training galaxies, and the standard deviation of the color-matched subset is used as the photo-$z$ uncertainty.

We have applied the nearest-neighbours color-matching photometric redshift estimator described in G18 to the simulated data. Compared to its application in G18, there are some minor differences in the application of this estimator to the Buzzard catalogue. First, we do not impose non-detections on galaxies with a magnitude fainter than the expected LSST 10-year limiting magnitude or bright enough to saturate with LSST: {\it all} of the photometry for all the galaxies in the test and training sets are used for this experiment. Second, as in G18 we do apply an initial cut in color to the training set before calculating the Mahalanobis distance in order to accelerate processing, and also use a magnitude pseudo-prior to improve photo-$z$ estimates, but for both we have used different cut-off values that are appropriate for the Buzzard galaxies' colors and magnitudes. Third, we set different parameters for the identification of the color-matched subset of training galaxies and the selection of a photometric redshift estimate. In G18 we used a percent point function (PPF) value of 0.68 to identify the color-matched subset of training galaxies and used the redshift of nearest neighbour in color-space as the photo-$z$ estimate. These choices work well when the desire is to obtain accurate photo-$z$ estimates for most test-set galaxies, but does not return a robust $p(z)$ in all cases -- especially for galaxies that are bright and/or have few matches in color-space. Since a robust estimate of $p(z)$ is desired for this work we make several changes to our implementation of the \textsc{CMNN} photo-$z$ estimator. We continue to use a percent point function of ${\rm PPF} = 0.95$ to generate the subset of color-matched training galaxies, but weight them by the inverse of their Mahalanobis distance. This weighting maintains some of the accuracy that was previously achieved by simply using the nearest neighbour in color-space. We then use the weights to create the $p(z)$ instead of having the redshift of each color-matched training-set galaxy count equally. To obtain a robust estimate of the $p(z)$ for galaxies with a small number of color-matched training set galaxies, when this number is less than $20$ the nearest $20$ neighbours in color-space are used instead, and we convolve the $p(z)$ with a Gaussian with a standard deviation of:

\begin{equation}
\sigma = \sigma_{\rm train} \sqrt{({\rm PPF}_{20}/0.95)^2 -1}
\end{equation}

\noindent to appropriately broaden it so that the $p(z)$ for these test galaxies represents the enlarged PPF value associated with it. Overall, these three changes will yield poorer accuracy photo-$z$ compared to those presented in G18, but they will all have significantly more robust estimates of the $p(z)$, particularly for the brightest test galaxies. This is sufficient for this work because, as described in G18, the goal of the \textsc{CMNN} photo-$z$ estimator was never to provide the ``best'' (or even competitive) estimates in the first place, given its reliance on a deep training set, but rather to provide a means for direct comparisons between LSST photometric quality and photo-$z$ estimates. With this work we show how the input parameters should be set in order to return robust $p(z)$ estimates in addition to point value estimates.


\subsubsection{Delight}
\label{sec:delight}
%(John Soo)

\textsc{Delight}\footnote{\url{https://github.com/ixkael/Delight}} \citep{Leistedt:17} infers photo-$z$'s by using a data-driven model of latent SEDs and a physical model of photometric fluxes as a function of redshift. Generally, machine learning methods rely on representative training data with similar band passes, while template based methods rely on a complete library of templates based on physical models constructed. \textsc{Delight} is constructed in attempt to combine the advantages and eliminate the disadvantages of both template-based and machine learning algorithms: it constructs a large collection of latent SED templates (or physical flux-redshift models) from training data, with a template SED library as a guide to the learning of the model. The advantage of \textsc{Delight} is that it neither needs representative training data in the same photometric bands, nor does it need detailed galaxy SED models to work.

This conceptually novel approach is done by using Gaussian processes operating in flux-redshift space. The posterior distribution on the redshift of a target galaxy is obtained via a pairwise comparison with training galaxies,

\begin{equation}
p(z|\mathbf{\hat{F}}) \approx \sum_i p(\mathbf{\hat{F}}|z,t_i)\, p(z|t_i)p(t_i),
\end{equation}
\noindent where $p(z|t_i)p(t_i)$ captures prior information about the redshift distributions and abundances of the galaxies, with $t_i$ denoting the galaxy template; while $p(\mathbf{\hat{F}}|z,t_i)$ is the posterior of noisy flux $\mathbf{\hat{F}}$ at redshift $z$. For each training-target pair, $p(\mathbf{\hat{F}}|z,t_i)$ is evaluated as follows:

\begin{equation} \label{eq:delight_noisy}
p(\mathbf{\hat{F}}|z,t_i) = \int p(\mathbf{\hat{F}}|\mathbf{F})\, p(\mathbf{F}|z,z_i,\mathbf{\hat{F}}_i)\, d\mathbf{F},
\end{equation}
where $p(\mathbf{\hat{F}}|\mathbf{F})$ is the likelihood function, it compares the noisy real flux $\mathbf{\hat{F}}$ with the noiseless flux $\mathbf{F}$ obtained from the linear combination of template models, carefully constructed to account for model uncertainties and different normalization of the same SED; while $p(\mathbf{F}|z,z_i,\mathbf{\hat{F}}_i)$ is the prediction of flux at a different redshift $z$ with respect to the training object with redshift $z_i$ and flux $\mathbf{\hat{F}}_i$. Eq.~\ref{eq:delight_noisy} is essentially the probability that the training and the target galaxies having the same SED but at a different redshift. The flux prediction $p(\mathbf{F}|z,z_i,\mathbf{\hat{F}}_i)$ of the training galaxy at redshift $z$ is modeled via a Gaussian process,

\begin{equation} \label{eq:delight_gp}
F_b \sim \mathcal{GP}\left( \mu^F,k^F \right),
\end{equation}
\noindent with mean function $\mu^F$ and kernel $k^F$, both imposed to capture expected correlations resulting from the known underlying physics (i.e., fluxes resulting from observing SEDs through filter response, and the SEDs being redshifted). The reader should refer to \citet{Leistedt:17} for further details.

In this study, all $100$ ordered Buzzard templates, as described in Section~\ref{sec:buzztemplates}, were used in \textsc{Delight}, and the Gaussian process was trained using the provided training sample. Photometric uncertainties from the inputs are propagated into the code, while non-detections for each band are set to the mean of the respective bands. The default settings of \textsc{Delight} were used, with the exception that the PDF bins were set to be linearly-spaced rather than logarithmic. In this study a flat prior is assumed.


\subsubsection{FlexZBoost}
\label{sec:flexzboost}
%(Ann Lee, Rafael Izbicki, Taylor Pospisil, Peter Freeman)

\textsc{FlexZBoost}\footnote{\url{https://github.com/tpospisi/flexcode};\\ \url{https://github.com/rizbicki/FlexCoDE}} \citep{Izbicki:17} is a particular realization of \texttt{FlexCode}, which is a general-purpose methodology for converting any conditional mean point estimator of $z$ to a conditional {\em density} estimator $f(z \vert \x)$, where $\x$ here represents our photometric covariates and errors.\footnote{Instead of $p(z)$, we use the notation $f(z \vert \x)$ to explicitly show the dependence on $\x$.} The key idea is to expand the unknown function $f(z \vert \x)$ in an orthonormal basis $\{\phi_i(z)\}_{i}$:

\begin{equation}
f(z|\x)=\sum_{i}\beta_{i }(\x)\phi_i(z). \label{eq::series_expansion}
\end{equation}
By the orthogonality property, the expansion coefficients are just conditional means

\begin{equation}
\beta_{i }(\x) =  \mathbb{E}\left[\phi_i(z)|\x\right] \equiv \int f(z|\x)   \phi_i(z) dz. \label{eq:FlexZBoost_coeffs}
\end{equation}
These coefficients can easily be estimated from data by regression.

In this paper, we use \textsc{xgboost}~\citep{Chen:16} for the regression part as these techniques scale well for massive data; it should however be noted that \textsc{FlexCode-RF} (also on GitHub), based on Random Forests, generally performs better for smaller data sets. As our basis, we choose a standard Fourier basis. There are two tuning parameters in our $p(z)$ estimate: (i) the number of terms, $I$, in the series expansion in Eq.~\ref{eq::series_expansion}, and (ii) an exponent $\alpha$ that we use to sharpen the computed density estimates $\widehat{f}(z|\x)$, according to $\widetilde{f}(z|\x) \propto \widehat{f}(z|\x)^\alpha$. We split the ``train data'' into a training set (85\%) and a validation set (15\%), and choose both $I$ and $\alpha$ in an automated way by minimizing the weighted $L_2$-loss function \citep[Eq. 5 in][]{Izbicki:17} on the  validation set.

Although {\tt FlexCode} offers a {\em lossless compression} of the photo-$z$ estimates (in this study, one can reconstruct $\widetilde{f}(z|\x)$ exactly at any resolution from estimates of the first 35 coefficients, Eq.~\ref{eq:FlexZBoost_coeffs}, for a Fourier basis $\{\phi_i(z)\}_{i}$), we discretize our final estimates into 200 bins linearly spaced in $0 < z < 2$ for easy comparison with other algorithms. Using a higher resolution may yield better results (with no added cost in storage).




%\subsubsection{Frankenz}
%\label{sec:frankenz}
%(seeking volunteers)
%
%\red{\textsc{Frankenz}\footnote{\url{https://github.com/joshspeagle/frankenz}} \cite{Speagle:frankenz} is...}
%

\subsubsection{GPz}
\label{sec:gpz}
%(Ibrahim Almosallam)

\textsc{GPz}\footnote{\url{https://github.com/OxfordML/GPz}} \citep{Almosallam:16a,Almosallam:15b} is a sparse Gaussian process based code, a scalable approximation of full Gaussian Processes \citep{Rasmussen:06}, with the added feature of being able to produce input-dependent variance estimations (heteroscedastic noise). The model assumes that the probability of the output $y$, the redshift, given the input $x$, the photometry, is $p(y|x)=\mathcal{N}\left(y|\mu(x),\sigma(x)^{2}\right)$. The mean function, $\mu(x)$, and the variance function $\sigma(x)^{2}$ are both linear combinations of basis functions that take the following form:

\begin{align}
f(x)=\sum_{i=1}^{m}\phi_{i}(x)w_{i},
\end{align}
where $\left\{\phi_{i}(x)\right\}_{i=1}^{m}$ and $\left\{w_{i}\right\}_{i=1}^{m}$ are sets of $m$ basis functions and their associated weights respectively. Basis function models (BFM), for specific classes of basis functions such as the sigmoid or the squared exponential, have the advantage of being universal approximators, i.e. there exist a function of that form that can approximate any function, with mild assumptions, to any desired degree of accuracy. The details on how to learn the parameters of the model and the hyper-parameters of the basis functions are described in \citet{Almosallam:15b}.

A unique feature in \textsc{GPz}, is that the variance estimate is composed of two terms each quantifying a different source of uncertainty. One term (the model uncertainty) reflects how much of the uncertainty is due to lack of training samples at the location of interest, whereas the second term (the noise uncertainty) reflects how much of the uncertainty is caused from observing many noisy samples at that location. Thus, the predictive variance can determine whether we need more representative samples or more precise samples for any particular location in the input space. \textsc{GPz} can also emphasize the importance of some samples as weights. This weight can be for example $|z_{\rm{spec}}-z_{\rm{phot}}|/(1+z_{\rm{spec}})$ to target the desired objective of minimizing the normalized redshift error or as a function of their probability in the test set relative to the training set in order to pressure the model to better fit samples that are rare in the training set but are expected to be abundant during testing.

The data is prepared for \textsc{GPz} by taking the log of the magnitude errors, decorrelating the data set using PCA and imputing the missing values using a simple linear model that estimates the missing variables given the observed ones. The log transformation helps to smooth the long tail distribution of the magnitude errors, which is more stable numerically and makes the optimization process unconstrained. The missing values are imputed by computing the mean of the training set $\mu$ and its covariance $\Sigma$, then we use the following equation to estimate the missing values from the observed ones %\[

\begin{equation}
x_{u} = \mu_{u}+\Sigma_{uo}\Sigma_{oo}^{-1}(x_{o}-\mu_{o}),
\end{equation}  %\]
where the subscript $o$ in $x_{o}$ indexes the \emph{observed} part of the input $x$, whereas the subscript $u$ indexes the \emph{unobserved} set (similarly for $\mu$ and $\Sigma$). This is the optimal expected value of the unobserved variables given the observed ones if the distribution is jointly Gaussian, note that if the variables are independent, i.e. $\Sigma_{uo}=0$, this will reduce to a simple average predictor.

We use the Variable Covariance (VC) option in \textsc{GPz} with 200 basis functions after we note that there is no significant increase in the performance on the validation set (using 80\%-20\% training-validation split) and with no cost-sensitive learning applied.  We do note that \textsc{GPz} is trained using a data set that includes galaxies fainter than those in the test set, which was truncated at $i<25.3$.  This mismatch in training vs.~test results in a slightly different distribution in galaxy photometry and redshift distribution between the training and test set, which most likely degraded results compared to a run where the training set more closely matched the test set.

\subsubsection{METAPhoR}
\label{sec:metaphor}
%(Stefano Cavuoti, Massimo Brescia, Giuseppe Longo)
%\textcolor{brown}{general review done on this section}
\textsc{METAPhoR} \citep[Machine-learning Estimation Tool for Accurate Photometric Redshifts,][]{Cavuoti:17} is a pipeline designed to provide photo-$z$ point estimates and a reliable PDF for machine learning (ML) based techniques. It includes pre- and post-processing phases, hosting a photo-$z$ prediction engine based on the Multi Layer Perceptron with Quasi Newton Algorithm (MLPQNA).
%, already validated on photo-$z$'s in several cases \citep{de_Jong:17,Cavuoti:17b,Cavuoti:15,Brescia:14,Brescia:13,Biviano:13}.
Due to its plug-in based modular nature, \textsc{METAPhoR} can be easily replaced by any other photo-$z$ prediction kernel, regardless of specific implementation, while still taking advantage of the overall code interface.

At a higher level, the pipeline mainly consists of three modules: (i) \textit{data pre-processing}, including a catalogue cross-matching sub-module \citep[based on the tool C3, ][]{Riccio:17}, a sub-module for photometric evaluation and error estimation of the multi-band catalogue used as Knowledge Base (KB), and a sub-module dedicated to the perturbation of the photometric KB, propaedeutic to the PDF estimation; (ii) \textit{photo-$z$ prediction}, which is the training/validation/test phase, producing the photo-$z$'s point estimates, based on a pre-selected ML method; (iii) \textit{PDF estimation}, specifically designed to calculate the PDF of the photo-$z$ estimation errors. The last module includes also a post-processing tool, providing some statistics on the produced point estimates and PDFs.

The photometry perturbation law is based on the formula $m_{ij} = m_{ij} + \alpha_{i}F_{ij}*u_{\mu=0,\sigma=1}$, where $\alpha_{i}$ is a user selected multiplicative constant (useful in case of multi-survey photometry), $u_{\mu=0,\sigma=1}$ is a random value from the standard normal distribution and $F_{ij}$ is a bimodal function (a constant function + polynomial fitting of the mean magnitude errors on the binned bands), heuristically tuned in such a way that the constant component is the threshold under which the polynomial function is considered too low to provide a significant noise contribution to the photometry perturbation.

As introduced, the photo-$z$ point estimate prediction engine of \textsc{METAPhoR} is based on the MLPQNA model, whose photo-$z$ regression training error, used by the quasi Newton learning rule, is based on the least square error and Tikhonov $L_{2}$-norm regularization \citep{Hofmann:18}.

As main prerogative, \textsc{METAPhoR} is able to provide a PDF for ML methods by taking into account the photometric errors provided with data, by running $N$ trainings on the same training set, or $M$ trainings on $M$ different random extractions from the KB. The different test sets, used to produce the PDF, are thus obtained by introducing a proper perturbation, parametrized from the photometric error distribution in each band, on the photometric data populating the original test set \citep{Brescia:18}.

For the present work since it was required to produce a redshift (and a PDF) for each object of the test set we decided to apply a hierarchical kNN to fill the missing detection, it goes without saying that for such points the reliability of PDFs and point estimation is lower. No cross validation has been used.


\subsubsection{SkyNet}
\label{sec:skynet}
%(J. Cohen-Tanugi and Hugo Tranin)

\textsc{SkyNet}\footnote{\url{http://ccpforge.cse.rl.ac.uk/gf/project/skynet/}} \citep{Graff:14} is a publicly available neural network software, based on a 2nd order conjugate gradient optimization scheme \citep[see][for further details]{Graff:14}. %It has been used efficiently for redshift PDF estimates \citep{Sanchez:14,Bonnett:15,Bonnett:16}.

%In the present work, we use \textsc{SkyNet} both as a regressor (for redshift point estimates) and a classifier (for PDF estimates).
The neural network is configured as a standard multilayer perceptron with three hidden layers and one input layer with $12$ nodes (the $6$ magnitudes and their errors).
%For the regressor, the output layer is a single linear node, while each hidden layer has $10$ nodes with a tanh activation function.
The classifier is laid out such that the hidden layers have 20:40:40 nodes each, all rectified linear units, and the output layer has $200$ nodes (corresponding to 200 bins for the PDF) activated with a ``softmax'' function so that they automatically sum to $1$.  While previous implementations of the code, such as \citet{Sanchez:14} and \citet{Bonnett:15} (see Appendix C.3), implement a ``sliding bin'' smoothing, no such procedure was used in this study.

To avoid over-fitting, a $30$ per cent fraction of the training set is used as validation, and the training is stopped as soon as the error rate begins to increase in the validation set. The weights are randomly initialized based on normal sampling. The error function is a standard chi-square function for the regressor, and a cross-entropy function for the classifier. Finally, the data are all whitened before processing, with magnitudes pegged to (45,45,40,35,42,42) and their errors pegged to (20,20,10,5,15,15) for $ugrizy$ filters, respectively.


\subsubsection{TPZ}
\label{sec:tpz}
%(Erfan Nourbakhsh)

\textsc{TPZ}\footnote{\url{https://github.com/mgckind/MLZ}} \citep[Trees for Photo-$z$,][]{Carrasco_Kind:13,Carrascokind:14} is a parallel machine learning algorithm that generates photometric redshift PDFs using prediction trees and random forest techniques.
The code recursively splits the input data (i.~e.~the training sample), into two branches, one after another, until a terminal leaf is created that meets a termination criterion (e.~g.~a minimum leaf size or a variance threshold).
Bootstrap samples from the training data and associated errors are used to build a set of prediction trees.  In order to minimize correlation between the trees, the data is divided in such a way that the highest information gain among the random subsample of features is obtained at every point. The regions in each terminal leaf node corresponds to a specific subsample of the entire data that possesses similar properties.

The training data is examined before running TPZ. Since TPZ does not handle non-detections (magnitudes flagged as 99.0), we replace these values with an approximation of the $1\sigma$ detection threshold, i.~e.~a signal to noise ratio of 1 in terms of magnitude uncertainty using the equation $dm = 2.5 ~ \log ( 1 + N/S )$ where $dm \sim 0.7526 ~ mag$ for $N/S=1$.  That is, for each band, we replace the non-detection with the magnitude corresponding to the error of 0.7526 from the error model forecasted for 10-year LSST data. The Out-of-Bag \citep{Breiman:84,Carrasco_Kind:13} cross-validation technique is used within TPZ to evaluate its predictive validity and determine the relative importance of the different input attributes. We employed this information to calibrate our algorithm.

In the present work, the LSST magnitudes $u,~g,~r,~i$ and colors $u-g,~g-r,~r-i,~i-z,~z-y$ and their associated errors are used in the process of growing 100 trees with a minimum leaf size of 5 (the $z$ and $y$ magnitudes did not show significant correlation with the redshift in our cross-validation, so we did not use them when constructing our trees). We partitioned our redshift space into 200 bins and smoothed each individual PDF with a smoothing scale of twice the bin size.
%there was a typo here that had 100 bins, I double checked, it is 200 in the final run.

\subsection{Simple Ensemble Estimator}\label{sec:method:trainz}
In addition to the main photo-$z$ algorithms described above we also include a very simple method as a pathological example.  For \trainz, as we will we call this simple estimator, we well define $p(z)$ as simply:

\begin{equation}
p(z) = \frac{1}{N_{ \mathrm train}}\sum_{\mathrm i=1}^{N_{\mathrm train}}z_{\mathrm train}
\end{equation}
That is, we simply set the redshift PDF of every galaxy equal to the normalized $N(z)$ of the training sample.  As the training sample is drawn from the same underlying distribution as the test sample, modulo small deviations due to sample size, the quantiles of the training and test distributions should be identical.  This is a wildly unrealistic estimator, as it assigns all galaxies, no matter their apparent magnitude, colour, or true redshift, the same redshift PDF, and is thus uninformative at the level of individual object redshifts, but is designed to perform very well for the ensemble of all objects.  If the training set was not representative, this estimator would produce biased results, and any attempts to break up the sample into tomographic bins will fail, as every galaxy has an identical $p(z)$.  We will discuss this method and cautions relative to metrics in Section~\ref{sec:caution}.
