\section{Methods}
\label{sec:pzcodes}

Here we summarize the twelve \pzpdf\ codes compared in this study, summarized in Table~\ref{tab:list_of_codes}, which include both established and emerging approaches in template fitting and machine learning.
Though not exhaustive, this sample represents codes for which there was sufficient expertise within the \lsstdesc\ Photometric Redshifts Working Group; the authors welcome interest from those outside \lsstdesc\ to have their codes assessed in future investigations that build upon this one.

\begin{table*}  %%% DATA TABLE %%%
\caption{List of \pzpdf\ codes featured in this study} \label{tab:list_of_codes}\resizebox{\textwidth}{!}{
\begin{tabular}{lll}
\hline
\bf Published code & \bf Type & \bf Public source code \\
\hline
\bpz~\citep{Benitez:00} 		   & template fitting	& \url{http://www.stsci.edu/~dcoe/BPZ/} \\
\eazy~\citep{Brammer:08}		   & template fitting & \url{https://github.com/gbrammer/eazy-photoz} \\
\lephare~\citep{Arnouts:99}	   & template fitting	& \url{http://www.cfht.hawaii.edu/~arnouts/lephare.html} \\
\annz~\citep{Sadeh:16}		     & machine learning	& \url{https://github.com/IftachSadeh/ANNZ} \\
\delight~\citep{Leistedt:17}   & machine learning & \url{https://github.com/ixkael/Delight} \\
\flexzboost~\citep{Izbicki:17} & machine learning & \url{https://github.com/tpospisi/flexcode}; \url{https://github.com/rizbicki/FlexCoDE}\\
%\textsc{Frankenz} 	& ML 	& \citet{Speagle:frankenz}	& \url{https://github.com/joshspeagle/frankenz} \\
\gpz~\citep{Almosallam:15b}	   & machine learning	& \url{https://github.com/OxfordML/GPz} \\
\metaphor~\citep{Cavuoti:17}   & machine learning	& \url{http://dame.dsf.unina.it}\\
\cmnn~\citep{Graham:17}        & machine learning & N/A \\
\skynet~\citep{Graff:14}       & machine learning & \url{http://ccpforge.cse.rl.ac.uk/gf/project/skynet/} \\
\tpz~\citep{Carrasco_Kind:13}	 & machine learning	& \url{https://github.com/mgckind/MLZ} \\
\hline
\trainz 	                             & machine learning	& See Section~\ref{sec:method:trainz} \\
\end{tabular}}
\end{table*}

We describe the algorithms and implementations of the model-based and data-driven codes in Sections~\ref{sec:templatecodes} and \ref{sec:trainingcodes} respectively.
% For each approach, we note how it accounts for photometric uncertainties, how it interprets negative fluxes, what its native output format of \pzpdf s is, and what, if any, preprocessing, including validation, of the prior information it performs.

\subsection{Template-based Approaches}
\label{sec:templatecodes}

We test three publicly available and commonly used template-based codes that share the standard physically motivated approach of calculating model fluxes for a set of template SEDs on a grid of redshift values and evaluating a \chisq\ merit function using the observed and model fluxes.

\subsubsection{LePhare}
\label{sec:lephare}
%(C\'ecile Roucelle, Eric Nuss, Johann Cohen-Tanugi)

\lephare \footnote{\url{http://www.cfht.hawaii.edu/~arnouts/lephare.html}}\citep[Photometric Analysis for Redshift Estimate,][]{Arnouts:99,Ilbert:06} matches observed colors with those predicted from a template set, which can be semi-empirical or entirely synthetic, directly according to the likelihood $\chi^{2}(z, T, A) \equiv \sum_{b}^{N_{\rm{filt}}} \left((F^{\rm{obs}}_{b} - A F^{\rm{mod}}_{b}(T, z)) / \sigma^{\rm{obs}}_{b}\right)^{2}$ of normalization factor $A$, template $T$, and redshift $z$.
In words, the likelihood is a sum of observed flux error $\sigma_{b}^{\rm{obs}}$-weighted squared differences between the observed flux $F^{\rm{obs}}_{b}$ and the normalized predicted flux $F^{\rm{mod}}_{b}(T, z)$ in $N_{\rm{filt}}$ photometric filters $b$, which is the \lsst\ $ugrizy$ filters in this case.
The reported \pzpdf\ is an arbitrary normalization of the likelihood evaluated on the output redshift grid.
%\textsc{LePhare} has been used to produce the COSMOS2015 photo-$z$ catalogue \citep{Laigle:16}.

Here we use \lephare-v 2.2 with the DC1 template set of Section~\ref{sec:buzztemplates}.

\subsubsection{BPZ}
\label{sec:BPZ}
%(Sam Schmidt)

\bpz \footnote{\url{http://www.stsci.edu/~dcoe/BPZ/}} \citep[Bayesian Photometric Redshift,][]{Benitez:00} determines the likelihood $p(C \mid z, T)$ of a galaxy's observed colours $C$ for a set of SED templates $T$ at redshifts $z$.
The \bpz\ likelihood is related to Equation~\ref{eq_temp_chi} by $p(C \mid z, T) \propto \exp[- \chi^{2} / 2]$.
Given a Bayesian prior $p(z, T \mid m_{0})$ over apparent magnitude $m_0$ and assuming that the SED templates are spanning and exclusive, \bpz\ constructs the redshift posterior $p(z \mid C, m_0)$ by marginalizing over all SED templates as in \citep[Eq.~3 from][]{Benitez:00}, corresponding to setting the parameter \texttt{PROBS\_LITE=TRUE} in the \bpz\ parameter file.
The \bpz\ prior is the product of an SED template proportion that varies with apparent magnitude $p(T \mid m_{0})$ and a prior $p(z \mid T, m_{0})$ over the expected redshift as a function of apparent magnitude and SED template.

Here we test \bpz-v 1.99.3 with the DC1 template set of Section~\ref{sec:buzztemplates}.
To keep the number of free parameters manageable, the DC1 template set is pre-sorted by the rest-frame $u$-$g$ colour and split into three broad classes of SED template, equivalent to the E, Sp and Im/SB types in .
The Bayesian prior term $p(T \mid m_{0})$ was derived directly from the DC1 training set, and the other term $p(z \mid T, m_{0})$ was chosen to be the best fit for the eleven free parameters of the functional form of \citet{Benitez:00}.
%For photo-$z$ point estimates we use the \texttt{Z\_B} output parameter.
Prior to running the code, the non-detection placeholder magnitude was replaced with an estimate of the one-$\sigma$ detection limit for the undetected band as a proxy for a value close to the estimated sky noise threshold.

\subsubsection{EAZY}
\label{sec:eazy}
%(Rongpu Zhou)

\eazy \footnote{\url{https://github.com/gbrammer/eazy-photoz}} \citep[Easy and Accurate Photometric Redshifts from Yale,][]{Brammer:08} extends the basic \chisq\ fit procedure that defines template-fitting approaches.
The algorithm models the observed photometry with a linear combination of template SEDs at each redshift.
The best-fit SED is found by simultaneously fitting one, two, or all of the templates via \chisq\ minimization, which is distinct from marginalizing across all templates.
The minimized \chisq\ likelihood at each redshift is then combined with an apparent magnitude prior to obtain the redshift posterior PDF.
We note that the utilization of the best-fit SED rather than a proper marginalization does not lead to the correct posterior distribution, an implementation issue that has now been identified and will be addressed by the developers in the future.
\eazy\ can account for uncertainty in the template set by adding in quadrature to the flux errors an empirically derived template error as a function of redshift.

The SED-independent apparent magnitude prior was derived empirically from the DC1 training set.
The \eazy\ architecture cannot accept a template set other than the same five basis templates employed by \texttt{k-correct} when constructing the DC1 catalogue.
However, \eazy\ does feature a flexible \texttt{all-templates} mode, which fits the photometric data with a linear combination of the five basis templates.
We set the template error to zero since the same templates were in fact used to produce the DC1 photometry.

\subsection{Training-based Approaches}
\label{sec:trainingcodes}

We compared nine data-driven \pz\ estimation approaches, eight of which are described in this section and one of which is discussed in Section~\ref{sec:trainz}.
Because the algorithms differ more from one another and the techniques are relative newcomers to the astronomical literature, we provide somewhat more detail about the implementations below.

% Some aspects of data treatment were left to the individual code runners, for example, whether and how to split the DC1 training set for validation.
% The codes considered treated
% Another key difference is the treatment of non-detections in one or more bands.
% Some codes choose to ignore a band, others replace the value with either an estimate for the detection limit, the mean of other values in the training set, or another default value.
% There are varying conventions among training-based codes for treatment of non-detections, and no one prescription dominates in the photo-$z$ literature.
% The specific choices for each code affect the results, and contribute to the implicit prior influencing their output.
% However, we remind the reader that only 2.0 per cent of our sample has non-detections, almost exclusively in the u-band, and thus should not dominate the code performance differences.

\subsubsection{ANNz2}
\label{sec:annz2}
%(John Soo)

\annz \footnote{\url{https://github.com/IftachSadeh/ANNZ}} \citep{Sadeh:16} employs several machine learning algorithms, including artificial neural networks (ANN), boosted decision tree, and k-nearest neighbour (KNN) regression.
In addition to accounting for errors on the input photometry, \annz\ uses the KNN-uncertainty estimate of \citet{Oyaizu:08} to quantify uncertainty in the choice of method over multiple runs.
Using the Toolkit for Multivariate Data Analysis with ROOT\footnote{\url{http://tmva.sourceforge.net/}}, it can return the results of running a single machine learning algorithm, a ``best'' choice of the results from simultaneously running multiple algorithms, or a combination of the results of multiple algorithms weighted by their method uncertainties averaged over multiple runs.
%\textsc{ANNz2} is capable of producing both photo-$z$ point estimates and redshift posterior probability distributions $p(z)$.
% It can also perform classifications, and supports reweighting between samples.
% \annz\ propagates the intrinsic uncertainty on the input parameters and the uncertainty in the machine learning method to the expected photo-$z$ solution, averaged over multiple runs weighted based on the performance of each run.

In this study, we used \annz-v.2.0.4 to output only the result of the ANN algorithm.
\Pzpdf s were produced by running an ensemble of 5 ANNs with 6 $ugrizy$ inputs, 2 hidden layers with 12 nodes each, and 1 output of redshift).
Each of the five ANNs was trained with different random seeds for the initialization of input parameters.
Additionally, all ANNs were trained on only a $i \leq 25.3$ subsample of the DC1 training set, and half of the training set was reserved for validation to prevent overfitting.
Undetected galaxies were excluded from the training set, and per-band non-detections in the test set were replaced with the mean magnitude in that band within the entire test set.

\subsubsection{Colour-Matched Nearest-Neighbours}
\label{sec:cmnn}
%(Melissa Graham)

The nearest-neighbours colour-matching photometric redshift estimator \citep[\cmnn,][]{Graham:17} uses a training set of galaxies with known redshifts that has equivalent or better photometry than the test set in terms of quality and filter coverage.
For each galaxy in the test set, \cmnn\ identifies a colour-matched subset of training galaxies using a threshold in the Mahalanobis distance $D_M$ in the space of available colours for the test set galaxy
% between the test galaxy and all training set galaxies:
% \begin{equation}
% D_{\rm M} = \sum^{N_{\rm colours}}\frac{(c_{\rm train}-c_{\rm test})^2}{(\delta c_{\rm test})^2}
% \end{equation}
% where c colour, $\delta c_{\rm test}$ is the measurement error on the colour, and $N_{\rm colours}$ is the total number of colors (i.e., in our case $u$-$g$, $g$-$r$, $r$-$i$, $i$-$z$, and $z$-$y$).
that defines the colour match set based on a value of the percent point function (PPF).
As an example, for $N_{\rm{filt}}=5$ with PPF$=0.95$, $95\%$ of all training galaxies consistent with the test galaxy will have $D_M < 11.07$.
Undetected bands are dropped, thereby reducing the effective $N_{\rm{filt}}$ for that galaxy.
The \pzpdf\ of a given test set galaxy is the normalized distribution of redshifts of its colour-matched subset of training set galaxies.

Here, we make two modifications to the implementation of \citet{Graham:17} to comply with the controlled experimental conditions.
First, we do not impose nondetections on galaxies fainter than the expected \lsst\ 10-year limiting magnitude or bright enough to saturate with \lsst's CCDs, instead using all of the photometry for the DC1 test and training sets.
Second, we apply the initial colour cut to the training set before calculating the Mahalanobis distance in order to accelerate processing and use a magnitude pseudo-prior as in \citet{Graham:17}, but for both we use cut-off values corresponding to the DC1 training set galaxies' colours and magnitudes.

We make an additional adaptation to enable the \cmnn\ algorithm to yield accurate \pzpdf s for all galaxies, as the original \citet{Graham:17} algorithm is optimized for \pz\ point estimates and is susceptible to less accurate \pzpdf s for bright galaxies or those with few matches in colour-space.
We use PPF$=0.95$ rather than PPF$=0.68$ to generate the subset of colour-matched training galaxies, whose redshifts are weighted by their inverse Mahalanobis distances of the when composing the \pzpdf\ rather than weighting all colour-matched training galaxies equally.
Additionally, when the number of colour-matched training set galaxies is less than 20, the nearest 20 neighbours in color-space are used instead, and the output \pzpdf\ is convolved with a Gaussian kernel of variance $\sigma_{\rm train}^{2}({\rm PPF}_{20}/0.95)^2 -1$ to account foe the corresponding growth of the effective PPF to include 20 neighbors.

\subsubsection{Delight}
\label{sec:delight}
%(John Soo)

\textsc{Delight}\footnote{\url{https://github.com/ixkael/Delight}} \citep{Leistedt:17} infers photo-$z$'s by using a data-driven model of latent SEDs and a physical model of photometric fluxes as a function of redshift.
Delight models the underlying latent SEDs as a linear combination of a set of pre-defined template SEDs, plus zero mean Gaussian processes with factorized kernels.
Generally, machine learning methods rely on representative training data with similar band passes, while template based methods rely on a complete library of templates based on physical models constructed.
\textsc{Delight} is constructed in attempt to combine the advantages and eliminate the disadvantages of both template-based and machine learning algorithms: it constructs a large collection of latent SED templates (or physical flux-redshift models) from training data, with a template SED library as a guide to the learning of the model.
The advantage of \textsc{Delight} is that it neither needs representative training data in the same photometric bands, nor does it need detailed galaxy SED models to work.

This conceptually novel approach is done by using Gaussian processes operating in flux-redshift space.
The posterior distribution on the redshift of a target galaxy is obtained via a pairwise comparison with training galaxies,

\begin{equation}
p(z|\mathbf{\hat{F}}) \approx \sum_i p(\mathbf{\hat{F}}|z,t_i)\, p(z|t_i)p(t_i),
\end{equation}
\noindent where $p(z|t_i)p(t_i)$ captures prior information about the redshift distributions and abundances of the galaxies, with $t_i$ denoting the galaxy template; while $p(\mathbf{\hat{F}}|z,t_i)$ is the posterior of noisy flux $\mathbf{\hat{F}}$ at redshift $z$.
For each training-target pair, $p(\mathbf{\hat{F}}|z,t_i)$ is evaluated as follows:
\begin{equation} \label{eq:delight_noisy}
p(\mathbf{\hat{F}}|z,t_i) = \int p(\mathbf{\hat{F}}|\mathbf{F})\, p(\mathbf{F}|z,z_i,\mathbf{\hat{F}}_i)\, d\mathbf{F},
\end{equation}
where $p(\mathbf{\hat{F}}|\mathbf{F})$ is the likelihood function, it compares the noisy real flux $\mathbf{\hat{F}}$ with the noiseless flux $\mathbf{F}$ obtained from the linear combination of template models, carefully constructed to account for model uncertainties and different normalization of the same SED; while $p(\mathbf{F}|z,z_i,\mathbf{\hat{F}}_i)$ is the prediction of flux at a different redshift $z$ with respect to the training object with redshift $z_i$ and flux $\mathbf{\hat{F}}_i$. Eq.~\ref{eq:delight_noisy} is essentially the probability that the training and the target galaxies having the same SED but at a different redshift.
The flux prediction $p(\mathbf{F}|z,z_i,\mathbf{\hat{F}}_i)$ of the training galaxy at redshift $z$ is modeled via a Gaussian process,

\begin{equation} \label{eq:delight_gp}
F_b \sim \mathcal{GP}\left( \mu^F,k^F \right),
\end{equation}
\noindent with mean function $\mu^F$ and kernel $k^F$, both imposed to capture expected correlations resulting from the known underlying physics (i.e., fluxes resulting from observing SEDs through filter response, and the SEDs being redshifted).
The reader should refer to \citet{Leistedt:17} for further details.

In this study, all $100$ ordered Buzzard templates, as described in Section~\ref{sec:buzztemplates}, were used in \textsc{Delight}, and the Gaussian process was trained using the provided training sample.
Photometric uncertainties from the inputs are propagated into the code, while non-detections for each band are set to the mean of the respective bands.
The default settings of \textsc{Delight} were used, with the exception that the PDF bins were set to be linearly-spaced rather than logarithmic. In this study a flat prior in magnitude/type is assumed.

\subsubsection{FlexZBoost}
\label{sec:flexzboost}
%(Ann Lee, Rafael Izbicki, Taylor Pospisil, Peter Freeman)

\textsc{FlexZBoost}\footnote{\url{https://github.com/tpospisi/flexcode};\\ \url{https://github.com/rizbicki/FlexCoDE}} \citep{Izbicki:17} is a particular realization of \texttt{FlexCode}, which is a general-purpose methodology for converting any conditional mean point estimator of $z$ to a conditional {\em density} estimator $f(z \vert \x)$, where $\x$ here represents our photometric covariates and errors.\footnote{Instead of $p(z)$, we use the notation $f(z \vert \x)$ to explicitly show the dependence on $\x$.}
The key idea is to expand the unknown function $f(z \vert \x)$ in an orthonormal basis $\{\phi_i(z)\}_{i}$:
\begin{equation}
f(z|\x)=\sum_{i}\beta_{i }(\x)\phi_i(z). \label{eq::series_expansion}
\end{equation}
By the orthogonality property, the expansion coefficients are just conditional means
\begin{equation}
\beta_{i }(\x) =  \mathbb{E}\left[\phi_i(z)|\x\right] \equiv \int f(z|\x)   \phi_i(z) dz. \label{eq:FlexZBoost_coeffs}
\end{equation}
These coefficients can easily be estimated from data by regression since $\mathbb{E}\left[\phi_i(z)|\x\right]$ is the regression of $\phi_i(z)$ (a transformation of $Z$) on $X$.

In this paper, we use \textsc{xgboost}~\citep{Chen:16} for the regression part; it should however be noted that \textsc{FlexCode-RF} (also on GitHub), based on Random Forests, generally performs better for smaller data sets.
As our basis, we choose a standard Fourier basis.
There are two tuning parameters in our $p(z)$ estimate: (i) the number of terms, $I$, in the series expansion in Eq.~\ref{eq::series_expansion}, and (ii) an exponent $\alpha$ that we use to sharpen the computed density estimates $\widehat{f}(z|\x)$, according to $\widetilde{f}(z|\x) \propto \widehat{f}(z|\x)^\alpha$.
We reserve 15\% of the training set data as a validation set, and choose both $I$ and $\alpha$ in an automated way by minimizing the weighted $L_2$-loss function \citep[Eq. 5 in][]{Izbicki:17} on the  validation set.
While the native storage format for \textsc{FlexCode} encodes the PDF using the coefficients shown in Equation~\ref{eq:FlexZBoost_coeffs}, to match the output format requested of other codes we discretize our final estimates into 200 bins linearly spaced in $0 < z < 2$.


%Although {\tt FlexCode} offers a lossless compression of the photo-$z$ estimates (in this study, one can reconstruct $\widetilde{f}(z|\x)$ exactly at any resolution from estimates of the first 35 coefficients, Eq.~\ref{eq:FlexZBoost_coeffs}, for a Fourier basis $\{\phi_i(z)\}_{i}$), we discretize our final estimates into 200 bins linearly spaced in $0 < z < 2$ for easy comparison with other algorithms. Using a higher resolution may yield better results (with no added cost in storage).



%\subsubsection{Frankenz}
%\label{sec:frankenz}
%(seeking volunteers)
%
%\red{\textsc{Frankenz}\footnote{\url{https://github.com/joshspeagle/frankenz}} \cite{Speagle:frankenz} is...}
%

\subsubsection{GPz}
\label{sec:gpz}
%(Ibrahim Almosallam)

\textsc{GPz}\footnote{\url{https://github.com/OxfordML/GPz}} \citep{Almosallam:16a,Almosallam:15b} is a sparse Gaussian process based code, a scalable approximation of full Gaussian Processes \citep{Rasmussen:06}, with the added feature of being able to produce input-dependent variance estimations (heteroscedastic noise).
The model assumes that the probability of the output $y$, the redshift, given the input $x$, the photometry, is $p(z|x)=\mathcal{N}\left(z|\mu(x),\sigma(x)^{2}\right)$. The mean function, $\mu(x)$, and the variance function $\sigma(x)^{2}$ are both linear combinations of basis functions that take the following form:
\begin{equation}
f(x)=\sum_{i=1}^{m}\phi_{i}(x)w_{i},
\end{equation}
where $\left\{\phi_{i}(x)\right\}_{i=1}^{m}$ and $\left\{w_{i}\right\}_{i=1}^{m}$ are sets of $m$ basis functions and their associated weights respectively.
Basis function models (BFM), for specific classes of basis functions such as the sigmoid or the squared exponential, have the advantage of being universal approximators, i.e. there exist a function of that form that can approximate any function, with mild assumptions, to any desired degree of accuracy.
The details on how to learn the parameters of the model and the hyper-parameters of the basis functions are described in \citet{Almosallam:15b}.

A unique feature in \textsc{GPz}, is that the variance estimate is composed of two terms each quantifying a different source of uncertainty.
One term (the model uncertainty) reflects how much of the uncertainty is due to lack of training samples at the location of interest, whereas the second term (the noise uncertainty) reflects how much of the uncertainty is caused from observing many noisy samples at that location.
Thus, the predictive variance can determine whether we need more representative samples or more precise samples for any particular location in the input space.
\textsc{GPz} can also emphasize the importance of some samples as weights. This weight can be for example $|z_{\rm{spec}}-z_{\rm{phot}}|/(1+z_{\rm{spec}})$ to target the desired objective of minimizing the normalized redshift error or as a function of their probability in the test set relative to the training set in order to pressure the model to better fit samples that are rare in the training set but are expected to be abundant during testing.

The data is prepared for \textsc{GPz} by taking the log of the magnitude errors, decorrelating the data set using PCA and imputing any missing magnitude values using a simple linear model that estimates the missing magnitudes given the observed ones.
The log transformation helps to smooth the long tail distribution of the magnitude errors, which is more stable numerically and makes the optimization process unconstrained.
The missing values are imputed by computing the mean of the training set $\mu$ and its covariance $\Sigma$, then we use the following equation to estimate the missing values from the observed ones %\[
\begin{equation}
x_{u} = \mu_{u}+\Sigma_{uo}\Sigma_{oo}^{-1}(x_{o}-\mu_{o}),
\end{equation}  %\]
where the subscript $o$ in $x_{o}$ indexes the \emph{observed} part of the input $x$, whereas the subscript $u$ indexes the \emph{unobserved} set (similarly for $\mu$ and $\Sigma$).
This is the optimal expected value of the unobserved variables given the observed ones if the distribution is jointly Gaussian, note that if the variables are independent, i.e. $\Sigma_{uo}=0$, this will reduce to a simple average predictor.
We use the Variable Covariance (VC) option in \textsc{GPz} with 200 basis functions after we note that there is no significant increase in the performance on the validation set (using 80\%-20\% training-validation split) and with no cost-sensitive learning applied.


\subsubsection{METAPhoR}
\label{sec:metaphor}
%(Stefano Cavuoti, Massimo Brescia, Giuseppe Longo)
%\textcolor{brown}{general review done on this section}
\textsc{METAPhoR} \citep[Machine-learning Estimation Tool for Accurate Photometric Redshifts,][]{Cavuoti:17} is a pipeline designed to provide photo-$z$ point estimates and a reliable PDF for machine learning (ML) based techniques.
It includes pre- and post-processing phases, hosting a photo-$z$ prediction engine based on the Multi Layer Perceptron with Quasi Newton Algorithm (MLPQNA).
%, already validated on photo-$z$'s in several cases \citep{de_Jong:17,Cavuoti:17b,Cavuoti:15,Brescia:14,Brescia:13,Biviano:13}.

\textsc{METAPhoR} includes data modules for pre-processing, photo-$z$ estimation, and PDF estimation, and post-processing.  The pre-processing includes a model for perturbation of the photometry that is employed in calculating the PDF of the photo-$z$ estimation errors.
The photometric perturbation is defined as:
\begin{equation}
 m_{ij} = m_{ij} + \alpha_{i}F_{ij}u_{\mu=0,\sigma=1}
\end{equation}
where $\alpha_{i}$ is a user selected multiplicative constant (useful in case of multi-survey photometry), $u_{\mu=0,\sigma=1}$ is a random value from the standard normal distribution and $F_{ij}$ is a bimodal function (a constant function + polynomial fitting of the mean magnitude errors on the binned bands), heuristically tuned in such a way that the constant component is the threshold under which the polynomial function is considered too low to provide a significant noise contribution to the photometry perturbation.

%At a higher level, the pipeline mainly consists of three modules: (i) \textit{data pre-processing}, including a catalogue cross-matching sub-module \citep[based on the tool C3, ][]{Riccio:17}, a sub-module for photometric evaluation and error estimation of the multi-band catalogue used as Knowledge Base (KB), and a sub-module dedicated to the perturbation of the photometric KB, propaedeutic to the PDF estimation; (ii) \textit{photo-$z$ prediction}, which is the training/validation/test phase, producing the photo-$z$'s point estimates, based on a pre-selected ML method; (iii) \textit{PDF estimation}, specifically designed to calculate the PDF of the photo-$z$ estimation errors. The last module includes also a post-processing tool, providing some statistics on the produced point estimates and PDFs.

%As introduced, the photo-$z$ point estimate prediction engine of \textsc{METAPhoR} is based on the MLPQNA model, whose photo-$z$ regression training error, used by the quasi Newton learning rule, is based on the least square error and Tikhonov $L_{2}$-norm regularization \citep{Hofmann:18}.

As main prerogative, \textsc{METAPhoR} is able to provide a PDF for ML methods by taking into account the photometric errors provided with data, by running $N$ trainings on the same training set, or $M$ trainings on $M$ different random extractions from the KB.
The different test sets, used to produce the PDF, are thus obtained by introducing a proper perturbation, parametrized from the photometric error distribution in each band, on the photometric data populating the original test set \citep{Brescia:18}.
For the present work since it was required to produce a redshift (and a PDF) for each object of the test set we decided to apply a hierarchical kNN to replace the missing detections with values based on their neighbors.
The reliability of PDFs and point estimation is lower. No cross validation has been used.

\subsubsection{SkyNet}
\label{sec:skynet}
%(J. Cohen-Tanugi and Hugo Tranin)

\textsc{SkyNet}\footnote{\url{http://ccpforge.cse.rl.ac.uk/gf/project/skynet/}} \citep{Graff:14} is a publicly available neural network software, based on a 2nd order conjugate gradient optimization scheme \citep[see][for further details]{Graff:14}. %It has been used efficiently for redshift PDF estimates \citep{Sanchez:14,Bonnett:15,Bonnett:16}.

%In the present work, we use \textsc{SkyNet} both as a regressor (for redshift point estimates) and a classifier (for PDF estimates).
The neural network is configured as a standard multilayer perceptron with three hidden layers and one input layer with $12$ nodes (the $6$ magnitudes and their errors).
%For the regressor, the output layer is a single linear node, while each hidden layer has $10$ nodes with a tanh activation function.
The classifier is laid out such that the hidden layers have 20:40:40 nodes each, all rectified linear units, and the output layer has $200$ nodes (corresponding to 200 bins for the PDF) activated with a ``softmax'' function so that they automatically sum to $1$.
While previous implementations of the code, such as \citet{Sanchez:14} and \citet{Bonnett:15} (see Appendix C.3), implement a ``sliding bin'' smoothing, no such procedure was used in this study.

To avoid over-fitting, a $30$ per cent fraction of the training set is used as validation, and the training is stopped as soon as the error rate begins to increase in the validation set.
The weights are randomly initialized based on normal sampling.
The error function is a standard chi-square function for the regressor, and a cross-entropy function for the classifier.
Finally, the data are all whitened before processing, with magnitudes pegged to (45,45,40,35,42,42) and their errors pegged to (20,20,10,5,15,15) for $ugrizy$ filters, respectively.

\subsubsection{TPZ}
\label{sec:tpz}
%(Erfan Nourbakhsh)

\textsc{TPZ}\footnote{\url{https://github.com/mgckind/MLZ}} \citep[Trees for Photo-$z$,][]{Carrasco_Kind:13,Carrascokind:14} is a parallel machine learning algorithm that generates photometric redshift PDFs using prediction trees and random forest techniques.
The code recursively splits the input data (i.~e.~the training sample), into two branches, one after another, until a terminal leaf is created that meets a termination criterion (e.~g.~a minimum leaf size or a variance threshold).
Bootstrap samples from the training data and associated errors are used to build a set of prediction trees.
In order to minimize correlation between the trees, the data is divided in such a way that the highest information gain among the random subsample of features is obtained at every point.
The regions in each terminal leaf node corresponds to a specific subsample of the entire data that possesses similar properties.

The training data is examined before running TPZ.
Since TPZ does not handle non-detections (magnitudes flagged as 99.0), we replace these values with an approximation of the $1\sigma$ detection threshold, i.~e.~a signal to noise ratio of 1 in terms of magnitude uncertainty using the equation $dm = 2.5 ~ \log ( 1 + N/S )$ where $dm \sim 0.7526 ~ mag$ for $N/S=1$.
That is, for each band, we replace the non-detection with the magnitude corresponding to the error of 0.7526 from the error model forecasted for 10-year LSST data.
The Out-of-Bag \citep{Breiman:84,Carrasco_Kind:13} cross-validation technique is used within TPZ to evaluate its predictive validity and determine the relative importance of the different input attributes.
We employed this information to calibrate our algorithm.

In the present work, the LSST magnitudes $u,~g,~r,~i$ and colours $u-g,~g-r,~r-i,~i-z,~z-y$ and their associated errors are used in the process of growing 100 trees with a minimum leaf size of 5 (the $z$ and $y$ magnitudes did not show significant correlation with the redshift in our cross-validation, so we did not use them when constructing our trees).
We partitioned our redshift space into 200 bins and smoothed each individual PDF with a smoothing scale of twice the bin size.
%there was a typo here that had 100 bins, I double checked, it is 200 in the final run.

\subsection{Simple Ensemble Estimator}
\label{sec:method:trainz}

In addition to the main photo-$z$ algorithms described above we also include a very simple method as a pathological example.
For \trainz, as we will we call this simple estimator, we well define $p(z)$ as simply:
\begin{equation}
p(z) = \frac{1}{N_{ \mathrm train}}\sum_{\mathrm i=1}^{N_{\mathrm train}}z_{\mathrm train}
\end{equation}
That is, we simply set the redshift PDF of every galaxy equal to the normalized $N(z)$ of the training sample.
This estimator is essentially a k nearest-neighbour estimator with k equal to the number of galaxies in the training sample.
As the training sample is drawn from the same underlying distribution as the test sample, modulo small deviations due to sample size, the quantiles of the training and test distributions should be identical, modulo fluctuations due to finite sample size.
This is a wildly unrealistic estimator, as it assigns all galaxies, no matter their apparent magnitude, colour, or true redshift, the same redshift PDF, and is thus uninformative at the level of individual object redshifts, but is designed to perform very well for the ensemble of all objects.
If the training set was not representative, this estimator would produce biased results, and any attempts to break up the sample into tomographic bins will fail, as every galaxy has an identical $p(z)$.
We will discuss this method and cautions relative to metrics in Section~\ref{sec:caution}.
