\section{Analysis}
\label{sec:metrics}
%(Alex Malz, Rongpu Zhou, Jeff Newman, Ofer Lahav)

%\red{Maybe it would be best to introduce the summary statistics (KS, CvM, AD) up front and then the ensemble functions and stacked n(z).}

% All code runners were asked to output redshift posterior estimates on 200 linear-spaced bins between redshifts 0 and 2.

The goal of this study is to evaluate the degree to which \pzpdf s of each method can be trusted for a generic probabilistic data analysis.
Obviously \pzpdf s must not take negative values and must integrate to unity over the range of possible redshifts.
The overloaded ``$p(z)$'' is a widespread abuse of notation that obfuscates this goal, so we will dedicate some attention to breaking it apart.
Galaxies have redshifts $z$ and photometric data $d$ drawn from a joint probability space $p(z, d)$ in nature.
As a result, each observed galaxy $i$ has a \textit{true posterior \pzpdf}\ $p(z \vert d_{i})$ as well as a true likelihood $p(d \vert z_{i})$.
There are a number of metrics that can be used to test the accuracy of a \pz\ posterior as an estimator of a true \pz\ posterior if the true \pzpdf\ is known.
However, the true \pzpdf\ is in general not accessible unless the photometry is in fact drawn from the true \pz\ likelihoods, a mock catalogue generation procedure that has not yet appeared in the literature and was not performed for this work.

Before describing the metrics appropriate to the DC1 data set, we outline the philosophy behind our choices.
A \pzpdf\ estimator derived by method $H$ must be understood as a posterior probability distribution
\begin{equation}
\hat{p}^{H}_{i}(z) \equiv p(z \vert d_{i}, I_{D}, I_{H}),
\end{equation}
conditioned not only on the photometric data $d_{i}$ for that galaxy but also on parameters encompassing a number of things that will differ depending on the method $H$ used to produce it, namely the assumptions $I_{H}$ necessary for the method to be valid and any inputs $I_{D}$ it takes as prior information, such as a template library or training set.
Because of this, direct comparison of \pzpdf s produced by different methods is in some sense impossible; even if they share the same prior information $I_{D}$, by definition they cannot be conditioned on the same assumptions $I_{H}$, otherwise they would not be distinct methods at all.

In this study, we isolate the effect of differences in prior information $I_{H}$ specific to each method by using a single training set $I_{D}^{\rm ML}$ for all machine learning-based codes and a single template library $I_{D}^{\rm T}$ for all template-based codes.
These sets of prior information are carefully constructed to be representative and complete, so we have $I_{D} \equiv I_{D}^{\rm ML} \equiv I_{D}^{\rm T}$ for every method $H$.
Thus, we are saying
\begin{equation}
\frac{\hat{p}_{i}^{H}(z)}{\hat{p}_{i}^{H'}(z)} \approx \frac{p(z \vert d_{i}, I_{H})}{p(z \vert d_{i}, I_{H'})},
\end{equation}
meaning that we assume comparisons of $\hat{p}_{i}^{H}(z)$ isolate the effect of the method used to obtain the estimator, which should enable interpetation of the differences between estimated PDFs in terms of the specifics of the method implementations.

The exact implementation of the metrics theoretically depends on the parametrization of the \pzpdf s, which may differ across codes and can affect the precision of the estimator \citep{Malz:qp}.
Even considering a single method under the same parametrization, such as the 200-bin $0 < z < 2$ piecewise constant function used here, the exact bin definitions will affect the result.
%\red{Note: Let's address how the gridding (endpoints and spacing) affects the metrics, from the email correspondence Re: [LSST-DESC-MEMBERS] Summary of Nov 29 PZ Telecon. (AIM)}
The piecewise constant format is chosen because of its established presence in the literature, and the choice of 200 bins was motivated by the approximate number of columns expected to be available for storage of \pzpdf s for the final \lsst\ Project tables.\footnote{See, e.~g.~the \lsst\ Data Products Definition Document, available at: \url{https://ls.st/dpdd}}
We will discuss the choice of \pzpdf\ parameterization further in Section~\ref{sec:results}.

This analysis is conducted using the \qp\footnote{\url{http://github.com/aimalz/qp/}}\ software package \citep{Malz:qp} for manipulating and calculating metrics of univariate PDFs.
We present the metrics of \pzpdf s that address our goals in the sections below.
Though the outmoded practice should not be encouraged, those seeking a connection to previous comparison studies will find metrics of common redshift point estimators derived from \pzpdf s in Appendix~\ref{sec:pointmetrics}.

\subsection{Metrics of an ensemble of photo-\mathinhead{$z$}{} posteriors}
\label{sec:qualmet}

Because \lsst's \pzpdf s will be used for many scientific applications, some of which require accuracy of each individual catalog entry, we consider several metrics that probe the population-level performance of the \pzpdf s.

\subsubsection{Probability integral transform (PIT)}
\label{sec:pit}
%(Alex Malz, Peter Freeman, Sam Schmidt)

The probability integral transform (PIT)
\begin{align}
\label{eq:pit}
\mathrm{PIT} &\equiv \int_{-\infty}^{z_{\rm true}} p(z) dz
\end{align}
is the Cumulative Distribution Function (CDF) of a \pzpdf\ evaluated at its true redshift, and the distribution of PIT values probes the average accuracy of the \pzpdf s of an ensemble of galaxies.
A catalogue of accurate \pzpdf s should have a PIT distribution that is uniform $U(0,1)$, and deviations from flatness are interpretable: overly broad \pzpdf s induce underrepresentation of the lowest and highest PIT values, whereas overly narrow \pzpdf s induce overrepresentation of the lowest and highest PIT values.
Catastrophic outliers with a true redshift outside the support of its \pzpdf\ have $\mathrm{PIT} \approx 0$ or $\mathrm{PIT} \approx 1$.

The PIT distribution has been used to quantify the performance of \pzpdf\ methods in the past \citep[e.~g.~][]{Bordoloi:10,Polsterer:16,Tanaka:17}.
\citet{Tanaka:17} use the histogram of PIT values as a diagnostic indicator of overall code performance, while \citet{Freeman:17} independently define the PIT and demonstrate how its individual values may be used both to perform hypothesis testing (via, e.g., the KS, CvM, and AD tests; see below) and to construct quantile-quantile plots.
%\red{We might want to divide this by redshift to get trends, as the overall PIT could be impacted by true n(z).
%For example, a PIT with nontrivial shape could be due to that behavior being uniform as a function of redshift or significant inaccuracy only in a small range of redshifts where $n(z)$ is large and a flat PIT elsewhere.}

\subsubsection{Quantile-quantile (QQ) plot}
\label{sec:qq}
%(Rongpu Zhou, Jeffrey Newman, Alex Malz)

A quantile is defined by partitioning a distribution into consecutive intervals containing equal amounts of probability, or equal numbers of objects in each interval in the case of a distribution of objects.
The quantile-quantile (QQ) plot serves as a graphical visualization for comparing two distributions, where the quantiles of one distribution are plotted against the quantiles of the other distribution.
%\blue{Include an equation to ensure quantile definition is crystal clear before building upon it for PIT.}\red{Is maybe the above text clear enough?  Text may be more clear than any equation I can come up with--SJS.}
The QQ plot provides an easy way to qualitatively assess the differences in various properties such as the moments of an estimating distribution relative to a true distribution.
%location, scale and skewness.

In this paper, QQ plots are used for two purposes: (1) for comparing $N(z)$ from \pzpdf s (estimated using Eq.~\ref{eq:stacked}) with the true $N(z)$, and (2) for assessing the overall consistency of an ensemble of \pzpdf s with their true redshifts on a population level, where the distribution of the PIT values (see previous section) is compared to a uniform distribution between $0$ and $1$.
Though the QQ plot contains very similar information to that shown in the PIT histogram plot, due to being the PIT being the derivative of the QQ, we include both forms for completeness and enhanced visual interpretability. %\red{is this enough reason to include both?--SJS}
%\aim{Maybe we should explain why we expect the distribution of CDFs to be uniform.}\red{Note: this is just describing the PIT.  I'm going to swap the order of PIT to come before this, and then say we are evaluating the distribution of PIT values.  I will also add why we expect this to be uniform--SJS}

\subsubsection{Conditional density estimation loss}
\label{sec:CDE_loss}
%(Alex Malz, Ann Lee)

With the conditional density estimation loss (CDE loss) we can compare how well different methods estimate individual PDFs for photometric covariates $\x$ rather than looking only at the ensemble distribution.
As in Section~\ref{sec:flexzboost}, we use the notation $f(z \vert \x)$ instead of $p(z)$ to explicitly show the dependence on the photometry $\x$.

%\aim{Perhaps this should be rewritten in the same notation used elsewhere in the paper.} \red{I agree, does someone who speaks both stats and astro want to take a shot at this?}
%\textcolor{cyan}{Ann: We are looking at individual PDFs so the dependence on $\x$ needs to be explicit; this is probably the most accurate notation as it shows both the conditioning clearly as well as whether $\x$ is fixed or random.}

The CDE loss is defined as
\begin{equation} \label{eq:cde-loss}
L(f, \widehat{f}) \equiv \int \int (f(z \mid \x) - \widehat{f}(z \mid \x))^{2} dz dP(\x) .
\end{equation}

This loss is the CDE equivalent of the RMSE in regression.
To estimate this loss we rewrite it as
\begin{equation} \label{eq:estimated-cde-loss}
L(f, \widehat{f}) = \mathbb{E}_\X \left[\int \widehat{f}(z \mid \X)^{2} dz\right] - 2 \mathbb{E}_{\X,Z}\left[\widehat{f}(Z \mid \X)\right] + K_{f},
\end{equation}
where upper-case letters denote random variables and lower-case the observed variables.  The first expectation is with respect to the marginal distribution of the covariates $\X$, the second expectation is with respect to the joint distribution of $\X$ and $Z$, and $K_{f}$ is a constant depending only upon the true conditional densities $f(z\mid \x)$.
For each method we can estimate these expectations as empirical expectations on the test or validation data \citep[Eq.~7 in][]{Izbicki:17b} without knowledge of the true densities.

%\subsubsection{Continuous Ranked Probability Score (CRPS)}
%\label{sec:crps}
%\red{The CRPS is used to assess the accuracy of two probabilistic models respectively.}

\subsection{Metrics over estimated probability distributions}
\label{sec:quantmet}

In tandem with the QQ and PIT metrics introduced above, we additionally compute the following metrics comparing the empirical CDF of a distribution to the true or expected distribution.
These metrics give a more quantitative measure of the departure from ideal than the more visual PIT histogram and QQ plot.
We compute metrics comparing the CDF of PIT values to the CDF of a Uniform distribution, and also compute the CDF of the true redshift distribution $N'(z)$ compared the $\hat{N}(z)$ distribution derived from summing the \pzpdf s as described in Eq.~\ref{eq:stacked}.

%\subsubsection{Root-mean-square error (RMSE)}
%\label{sec:rmse}
%%(Alex Malz)%
%
%We employ the familiar root-mean-square error
%\begin{equation}
%\mathrm{RMSE} \equiv \sqrt{\int_{-\infty}^{\infty}\left(\hat{f}(z)-f'(z)\right)^{2}\,dz} .
%\end{equation}
%%may be used for two purposes in this analysis.
%%We calculate it between the true redshift distribution $N'(z)$ and each estimator $\hat{N}(z)$ (derived by Eq.~\ref{eq:stacked}) as well as between the ideal QQ and PIT curves from each method studied and the ideal curves.
%Though this metric does not account for the fact that the redshift distribution function is, in fact, a probability distribution, it can still be interpreted as a measure of the integrated difference between the estimated distribution and the true distribution.%, and it can be used to quantify the otherwise qualitative metrics.
%

\subsubsection{Kolmogorov-Smirnov (KS) and related statistics}
\label{sec:ks}
%(John Soo, Rongpu Zhou)

The \textit{Kolmogorov-Smirnov statistic} $N_{\rm KS}$ is the maximum difference between $F_{\rm phot}(z)$ and $F_{\rm spec}(z)$, the CDFs of the photo-$z$ and spectroscopic redshift respectively:
\begin{equation}
N_{\rm KS} \equiv \max_{z}\left( \left| F_{\rm phot}(z) - F_{\rm spec}(z) \right| \right).
\end{equation}
%The KS test quantifies whether the two redshift distributions are drawn from a similar distribution, independent of binning.
The KS test quantifies the similarity between two distributions, independent of binning.
A lower $N_{\rm KS}$ value corresponds to more similar distributions.
%We calculate two KS test statistics, one to test the distribution of $p(z)$ values, and one to test the stacked estimator $\hat{n}(z)$ of the redshift distribution.
%The $p(z)$ test compares the CDF of the PIT values described in the previous section to that of a uniform distribution.
%The $n(z)$ test compares the CDF of the distribution formed by stacking the $p(z)$ for every object in our test set against the CDF of the corresponding true redshifts.
%In this case a lower $N_{\rm KS}$ value means that the photo-$z$ distribution fits the spectroscopic redshift distribution well. $F_{\rm phot}(z)$ is calculated by stacking the PDFs of every single object in the sample, producing a smooth cumulative $n(z)$ to be compared with $F_{\rm spec}(z)$.

We also consider two variants of the KS statistic: the Cramer-von Mises (CvM) and Anderson-Darling (AD) statistics.
The CvM statistic is similar to the KS statistic as it is also computed from the distance between the measured CDF and the ideal CDF, but instead of the maximum distance, the CvM statistic
\begin{equation}
\label{eq:cvm}
\omega^2 \equiv \int_{-\infty}^{+\infty}\big(F_{\mathrm{meas.}}(x) - F_{\mathrm{ideal}}(x)\big)^2\mathrm{d}F_{\mathrm{ideal}}
\end{equation}
is the average of the distance squared.
% In this paper we use the CvM statistic to evaluate the performance of both $n(z)$ and $p(z)$.

The AD statistic
\begin{equation} \label{eq:ad}
A^2 \equiv N_{tot}\int_{-\infty}^{+\infty} \frac{\big(F_{\mathrm{meas.}}(x) - F_{\mathrm{ideal}}(x)\big)^2} {F_{\mathrm{ideal}}(x) (1-F_{\mathrm{ideal}}(x))}\mathrm{d}F_{\mathrm{ideal}}
\end{equation}
is a weighted version of the CvM statistic, making it more sensitive to the tails of the distribution, where $N_{tot}$ is the sample size.
%The KS statistic and the CvM and AD statistics related to it are all used to evaluate the performance of both $\hat{n}(z)$ and the QQ and PIT of the CDFs of the ensemble of photo-$z$ interim posteriors.

%\subsubsection{Kullback-Leibler Divergence (KLD)}
%\label{sec:kl}
%(Alex Malz)
%
%The KLD is a measure of the loss in information when using a probability distribution that is an estimate of a known true probability distribution, so we use
%\begin{equation}
%\mathrm{KLD} = \int_{-\infty}^{\infty}n'(z)\log\left[\frac{n'(z)}{\hat{n}(z)}\right]dz
%\end{equation}
%as a metric on the stacked redshift distribution estimator of Eq.~\ref{eq:stacked}. Noting the assumptions made in using Eq.~\ref{eq:stacked}, we say that a photo-$z$ PDFs method whose stacked redshift distribution estimator $\hat{n}(z)$ more closely resembles the true redshift distribution $n'(z)$ is desired, i.e. we seek to minimize the KLD.

\subsubsection{Metrics of a science-motivated quantity}
\label{sec:moments}
%(Alex Malz)

The most popular application of \pzpdf s is the estimation of the overall redshift distribution $N(z)$, the true value of which is known for the DC1 data set and will be denoted as $\tilde{N}(z)$.
In terms of the prior information provided to each method, the true redshift distribution satisfies the tautology $\tilde{N}(z) = p(z \vert I_{D})$ due to our experimental set-up; because the DC1 training and template sets are representative and complete, $I_{D}$ represents a prior that is also equal to the truth.
In this ideal case of complete and representative prior information, the method that would give the best approximation to $\hat{N}(z)$ would be one that neglects all the information contained in the photometry $\{d_{i}\}_{N_{tot}}$ and gives every galaxy the same \pzpdf\ $\hat{p}_{i}(z) = \tilde{N}(z)$ for all $i$; the inclusion of any information from the photometry would only introduce noise to the optimal result of returning the prior.
This is the exact estimator, \trainz, that we have described in Section~\ref{sec:method:trainz}, and which will serve as an experimental control.

Though alternatives exist \citep{Malz:chippr}, ``stacking'' according to
\begin{equation}
  \label{eq:stacked}
  \hat{N}^{H}(z) \equiv \frac{1}{N_{tot}}\ \sum_{i}^{N_{tot}}\ \hat{p}^{H}_{i}(z)
\end{equation}
is the most widely accepted method for obtaining $\hat{N}^{H}(z)$ as an estimator of the redshift distribution from \pzpdf s derived by a method $H$.
Though we do not endorse the use of the stacked estimator of the redshift distribution, we use it under the untested assumption that the response of our metrics of $\hat{N}^{H}(z)$ will be analogous to the same metrics applied to a principled estimator of the redshift distribution.

We additionaly calculate the first three moments of the estimated redshift distribution $\hat{N}^{H}(z)$ for each code and compare them to the moments of the true redshift distribution $N'(z)$.
The $m^{\mathrm{th}}$ moment of a distribution is defined as
\begin{equation}
\langle z^{m}\rangle \equiv \int_{-\infty}^{\infty}z^{m}N(z)dz .
\end{equation}
Here, we use the moments of the stacked estimator of the redshift distribution function as the basis for a metric.
The closer the moments of $\hat{N}(z)$ for a \pzpdf\ method are to the moments of the true redshift distribution function $N'(z)$, the better the \pzpdf\ method is a estimating the overall redshift distribution ``shape''.
