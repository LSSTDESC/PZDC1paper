\section{Discussion and Future Work}
\label{sec:discussion}
%(Sam Schmidt, Ofer Lahav, Jeff Newman, Alex Malz, Eve Kovacs, Tony Tyson, Tina Peters)
In this paper we presented results evaluating the computation of individual galaxy photometric redshift PDFs for twelve photo-$z$ codes.  As discussed in Section~\ref{sec:metrics} each $p(z)$ should accurately reflect the relative likelihood as a function of redshift for each galaxy in an informative way; that is, the estimates should provide useful information per individual galaxy, not just the ensemble.  All codes were provided a set of representative training data and tested on an idealized set of model galaxies with high signal-to-noise and photometry with no confounding effects due to blending, instrumental effects, the night sky, or other complications included.  The goal was not to determine a ``best'' photo-$z$ code: in many ways, this was a baseline test of a ``best case scenario'' to predict the expected photo-$z$ performance if a stage IV dark energy survey was to obtain complete training samples and perfectly calibrated their multi-band photometry.  Given these idealized conditions, any deficiencies observed in a photo-$z$ code's performance should be a cause for concern, and may be evidence of a problem with either/both of the specific code implementation or the underlying algorithm.  In order to meet the stringent LSST goals for photo-$z$ performance, identifying and correcting such problems is an important first step before tackling more realistic data in future challenges, and codes that do not perform well may not be worth pursuing in future challenges.  Many of the codes tested performed well; however, several did not meet the stringent goals that have been laid out for LSST photometric redshift performance for individual galaxies, as laid out in the LSST SRD (See Section~\ref{sec:intro}.  This is a cause for concern, given the idealized conditions, and the individual code responses will be studied in detail moving forward.  If methods can not reach the goals on idealized data, then they will almost surely not meet those same goals when the more complex problems that we expect to arise from real LSST data are included.  The results presented in this paper enable an evaluation of which algorithms are the most promising moving forward, and potentially point to implementation choices or mistakes which could be improved or corrected in others.

One obvious trend in several of the codes tested was an overall over or underprediction of the widths of $p(z)$, as evidenced by the QQ plots and PIT histograms shown in Fig.~\ref{fig:pitqq}.  A more careful tuning of bandwidth or smoothing during the validation process appears to be necessary for many of the machine learning based codes in order to improve the accuracy of $p(z)$.  For narrow peaked $p(z)$ the parameterization of the PDF as evaluated on a fixed redshift grid could also have contributed to some overestimates of $p(z)$ width simply due to the finite resolution.  After evaluating results such as those presented in \citet[]{Malz:qp}, in future analyses we plan to switch from a fixed grid to quantile-based storage of $p(z)$ in order to more efficiently and accurately store redshift PDF results.

Another important factor to keep in mind when examining the results presented in this paper is the fact that they are at some level dependent on the metrics that we aim to optimize: in this case code participants were asked to submit their optimal measures of an accurate $p(z)$, so participants used the training/validation data to optimize their codes accordingly.  Had we, instead, asked for an optimal $\hat{N(z)}$ the resulting metrics would be different for most, if not all, of the codes, as they would optimize toward a different goal.  Specific metric choice can affect which codes are among the ``best'' codes.  As stated earlier, there are cosmological science cases that require either individual galaxy photo-$z$ measures, or ensemble $\hat{N}(z)$ measures.  We must be aware of that the optimal method for one is not necessarily optimal for the other, and in fact several photo-$z$ algorithms may be necessary in the final cosmological analysis in order to satisfy the requirements of all science use cases.  The example of the simple \trainz\ estimator described in Section~\ref{sec:caution} shows a simple model with a $p(z)$ that is unrealistic for individual objects can still score very well on many of our metrics.  It is important to look at {\it all} metrics, and keep in mind what information each metric conveys.
%\red{mention 'null' and how 'dumb' algorithms can score well on metrics, need to keep all metrics in mind}
%\aim{[reiterate the meaning of $p(z)$ and the goal -- emphasize finding ``best'' code in terms of impact of assumptions underlying the method \textit{or} establishing methodology for the realistic case of biased prior information and other science goals]}
%\red{[the challenges faced in this project]}
%\red{mention z$<$2, missing some important degeneracies, new sim will go to higher z}
%\red{no quality cuts included, could identify outliers, but could also induce biases}
%\red{[how optimization defined has impact, here we optimized p(z), not n(z), could be science-case specific if stringent requirements are needed, though that may be a computational/storage challenge if too many use cases needed.]}
%\red{some discussion of output parameterization, limitations of 200 point fixed grid, in future switch to something like quantiles (cite qp paper)}
We re-emphasize that the dataset tested was quite idealized, and discuss enhancements that will be added in future simulations to test photo-$z$ codes on increasingly realistic conditions in the following section.

\subsection{Future work}
\label{sec:futurework}
The work presented in this paper is only the first step in characterizing current photo-$z$ codes and moving toward an improved photometric redshift estimator.  This initial paper explored code performance in idealized conditions with perfect catalog-based photometry and representative training data.  As mentioned in Section~\ref{sec:stackedmetrics} for the stacked $N(z)$ metrics we examined only the entire galaxy population with no selections in either photo-$z$ ``quality" or redshift.  The cosmological analyses for weak lensing and large scale structure based measures plan to break galaxy samples into tomographic redshift bins, using photo-$z$ $p(z)$ to infer the redshift distribution for each bin.  The specific selection used to determine these bins, both algorithmically and the specific bin boundaries, could induce biases due to indirect selections inherent in the photo-$z$ or other bin selection parameters.  The effects of tomographic bin selection will be explored in a dedicated future paper, including propagation of redshift uncertainties in a set of fiducial tomographic redshift bins in order to estimate impact on cosmological parameter estimation.

%\red{[are there any references for this?  I remember Gary Bernstein talking about this at a photo-z workshop in Japan, but I don't know that it was published.  I believe Michael Troxel has discussed this as well.]}
%\item \red{cosmological parameters in DC2}
%\item \red{mention z$<$2, missing some important degeneracies, new sim will go to higher z}
%\red{no quality cuts included, could identify outliers, but could also induce biases}

In future papers a focus of the {\it LSST Dark Energy Science Collaboration Photo-z Working Group} will be to add more and more complexity to our simulated data in order to test photo-$z$ algorithms in increasingly realistic conditions.  The most pressing concern is the impact of incomplete spectroscopic training samples.  The SEDs for the galaxy sample in this paper were constructed form linear combinations of five basis SED templates.  Future simulations will also include more complex SED information, with a more realistic range of physical properties, and the inclusion of AGN effects, a more insidious problem, where AGN features may not be apparent, but the colors and other host galaxy properties are perturbed relative to galaxies with an inactive nucleus.  In such cases, the presence of the AGN may induce a bias if the template SEDs or empirical datasets do not include low-level AGN counterparts.

As discussed extensively in \citet{Newman:2015} a representative set of spectroscopically confirmed galaxies spanning the full range of both redshift and apparent magnitude is necessary as a training set to characterize the mapping from broad-band fluxes to photometric redshifts.
%However, due to a combination of factors due to both the galaxy SEDs and limitations of spectrographic instruments, redshift samples are known to be systematically incomplete, where certain galaxy types and redshift intervals fail to yield a redshift even at the longest integration times on current and near-future instruments.  The more representative the training data, the better the performance of photo-$z$ algorithms will be.
Current and upcoming surveys are putting significant effort into obtaining these training samples \citep[e.~g.\,][]{Masters:2017}, however we still expect significant incompleteness for LSST-like samples, particularly at faint magnitudes.  We plan to produce a realistically incomplete training set of spectroscopic galaxies, modeling the performance of spectrographs, emission-line properties, and expected signal-to-noise to determine which galaxies will fail to yield a secure redshift.  In addition to outright redshift failures we will model the inclusion of a small number of falsely identified secure redshifts where misidentified emission lines or noise spikes cause an incorrect redshift solution to be marked as a high quality identification.  Even sub-per cent level contamination by false redshifts can impact photo-$z$ solutions at levels comparable to the stringent  requirements of some LSST science cases.
We expect different systematics to occur in different photo-$z$ codes in response to training on incomplete data, particularly some of the machine learning methods.  The response of the codes will inform future directions of code development.

The underlying dataset limited this work to a maximum redshift of $z=2$.  LSST imaging after 10 years of observations will include a significant number of $z>2$ galaxies in expected cosmology samples, and their inclusion does have potential significant implications for photo-$z$ measures: the high redshift galaxies lie at fainter apparent magnitudes and can have anomalous colours due to evolution of stellar populations and the shift to rest-frame magnitudes probing UV features of the underlying SED.  More importantly, one of the most common ``catastrophic outlier'' degeneracies observed in deep photometric samples occurs when the Lyman break is mistaken for the Balmer break, leading to multiple redshift solutions at $z\sim0.2-0.3$ and $z\sim2-3$ \citep{Massarotti:2001}.  This degeneracy, along with other potential degeneracies, are currently not covered by the limited redshift range of this initial paper, which could mean that we are not probing the full range of potential extreme outlier populations and how our photo-$z$ estimators respond to them. Extending simulations to include the high-redshift galaxy population will be a priority in future data challenges.

This initial paper explored a data set that was constructed at the catalog level, with no inclusion of the complications that come from measuring photometry from images.  Future data challenges will move to catalogs constructed from mock images, including effects that will have great impact on photo-$z$ measurements, which will naturally include the complications of object blending, sensor effects, different observing conditions, amongst others.  Object blending will be a major area of investigation, as the mixing of flux from multiple objects and the resultant change in measured colours is predicted to affect a large fraction of LSST galaxies \citep{Dawson:2016}, and will be one of the major contributing systematics for photo-$z$'s. 

Finally, while this paper and future papers discussed above focus on photometric redshift codes and estimating accurate $p(z)$ from training data, we plan a separate, but complementary, project to examine calibration of the resultant redshifts via spatial cross-correlations \citep{Newman:2008}, which will be explored in a separate set of papers.  The overarching plan describing everything laid out in this section is described in more detail in the LSST DESC Science Roadmap (see Footnote in Section~\ref{sec:intro}).  These plans will require significant effort, but they are necessary if we are to make optimal use of the LSST data for astrophysical and cosmological analyses.   
