\section{Introduction}
\label{sec:intro}

%(Sam Schmidt, Ofer Lahav, Jeff Newman, Alex Malz, John Soo, Tony Tyson)

%\jan{Is the intention to keep these parenthetical author sets?  I find it a bit offputting in the text.  It might be better to put this for sections in the acknowledgements section, and for individual codes/statistics specify who ran/developed it with a sentence.}\red{I was just about to comment these out before finalizing the internal review draft of the paper. They are in place to remind me to contact a few people who still have not filled out their authors.csv info.}

Large-scale photometric galaxy surveys are entering a new era with currently or soon-to-be running Stage III and Stage IV dark energy experiments like the Dark Energy Survey \citep[DES,][]{Abbott:05}, the Kilo-Degree Survey \citep[KiDS,][]{de_Jong:13}, Hyper Suprime-Cam (HSC) Survey \citep[]{Aihara:2018a,Aihara:2018b}, Large Synoptic Survey Telescope \citep[LSST,][]{Abell:09}, Euclid \citep{Laureijs:11}, and Wide-Field Infrared Survey Telescope \citep[WFIRST,][]{Green:12}.
The move to imaging based surveys, rather than spectroscopic based, for cosmological measurements makes proper understanding of photometric redshifts (``photo-$z$'s'') of paramount importance, as cosmological distance measures for statistical samples are directly dependent on photo-$z$ measurements.

The unprecedented sample size of LSST galaxies, expected to number several billion for the main cosmological sample, necessitates stringent constraints on photo-$z$ accuracy if systematic errors are not to dominate the statistical errors.
The LSST Science Requirements Document (SRD)\footnote{available at \url{https://docushare.lsstcorp.org/docushare/dsweb/Get/LPM-17}} lists the photometric redshift goals for a magnitude limited sample with $i<25$ as: root-mean-square error with a goal of $\sigma_z<0.02(1+z)$; $3\sigma$ ``catastrophic outlier'' rate below $10\%$; bias below $0.003$\footnote{
Note that at the time the SRD was written, these goals were stated in terms of a photo-$z$ point estimate for each galaxy, as was standard in many previous studies, while in this paper we emphasize the importance of using a full photo-$z$ PDF.}.  The LSST Dark Energy Science Collaboration (LSST-DESC) developed a separate Science Requirements Document \citep{Mandelbaum:2018}, which forecasts the constraining power of five cosmological probes using somewhat conservative assumptions to define requirements on systematic errors for several measurements.  These include even more stringent requirements on photometric redshift performance than those included in the LSST SRD, though the initial LSST-DESC requirements on tomographic bin populations.  The tremendous size of LSST's galaxy catalogue will be enabled by its exceptional depth, pushing to fainter magnitudes and deeper imaging and including galaxies of lower luminosity and higher redshift than ever before.
The inclusion of these populations introduce major physical degeneracies, for example the Lyman break/Balmer break degeneracy, that were not present in the populations covered in shallower large area surveys like the Sloan Digital Sky Survey \citep[SDSS,][]{York:00} and the Two Micron All Sky Survey \citep[2MASS,][]{Skrutskie:06}.  These issues are not unique to LSST, and are present in Stage III Dark Energy surveys; however,
in order to meet the demanding error budgets of Stage IV projects such as LSST and LSST-DESC it will be necessary to fully characterize those degeneracies wherein multiple redshift solutions have comparable likelihood to per cent level accuracy.

There is often a desire to have a single valued ``point-estimate'' redshift for an individual galaxy.  However, the complex, non-linear (and often non-unique) nature of the mapping between broad band fluxes and redshift means that a single value is unable to capture the full redshift information encoded in a galaxy's magnitudes.  For example, a common point-estimate for a template-based method is taking the highest likelihood solution as the point photo-$z$.  A single valued redshift ignores degenerate redshift solutions of lower probability, potentially biasing photometric redshift estimates both for individual galaxies and ensemble distributions.  Storing more information is necessary, most often photo-$z$ codes output the redshift probability density function, also often referred to as $p(z)$, describing the relative likelihood as a function of redshift. %\red{should we move some of AIM's p(z) vs p(z|d,I) to here?}
Early template methods such as \citet{Fernandezsoto:99} converted relative $\chi^2$ values of template spectra to likelihoods to estimate $p(z)$.
Soon after, codes such as \citet{Benitez:00} added a Bayesian prior and output a posterior probability distribution.
While many early machine learning based algorithms focused on a point-estimate, \citet{Firth:03} used a neural net with 1000 realizations scattered within the photometric errors to estimate a $p(z)$.
As more groups began to employ photometric redshifts in their cosmological analyses, there was a realization that point-estimate photo-$z$'s were inadequate for precision cosmology measurements \citep{Mandelbaum:2008}.
From around this point onward, most photo-$z$ algorithms have attempted to implement some estimate of the overall redshift probability in their outputs, and some surveys began supplying a full $p(z)$ rather than a simple redshift point-estimate and error \citep[e.~g.~][]{de_Jong:17}.

For cosmological measurements, certain science cases require redshift information on individual objects, e.~g.~identification of host galaxy redshift for supernova classification, or identifying potential cluster membership.
Other science cases seem to need only ensemble redshift information; for instance current weak lensing techniques require only the overall redshift distribution $N(z)$ for tomographic redshift samples.  However, recent data-driven techniques employing hierarchical Bayesian or Gaussian Process methods have emerged that calibrate redshift distributions using individual $p(z)$ estimates \citep[e.~g.~][]{Sanchez:2018}.  These methods assume that the $p(z)$ for each galaxy is an accurate PDF, and such methods break down if this assumption is invalid.  Thus, even methods that seem to need only ensemble $N(z)$ may actually require accurate $p(z)$ in order to meet stringent survey requirements.
%In the case of the multiple types of probes of cosmology enabled by the
%LSST cosmology sample of several billion galaxies, the number of redshift bins and their photo-$z$ requirements vary with the specific probe;  2-point angular correlations benefit from many bins, while weak lensing probes do not (due to the wide lensing kernel).
Large photometric surveys such as LSST must develop algorithms that simultaneously meet the needs of all science cases.
In order to meet these ambitious goals for photo-$z$ accuracy, every aspect of photo-$z$ estimation will have to be optimized: the algorithms employed, both template and machine-learning based (both in design and implementation); the spectroscopic data used as a training set for machine learning algorithms or to estimate template sets and train Bayesian priors; and probabilistic catalogue compression schemes that balance information retention against limited storage resources.

There are numerous techniques for deriving photo-$z$ PDFs from photometry, yet no one method has yet been established as clearly superior.
Quantitative comparisons of photo-$z$ methods have been made before.
The Photo-$z$ Accuracy And Testing \citep[PHAT,][]{Hildebrandt:10} effort focused on point estimates derived from many photometric bands.  \citet{Rau:2015} introduced a new method for improving redshift PDFs using an ordinal classification algorithm.
DES compared several codes for point estimates \citep{Sanchez:14} and a summary statistic of photo-$z$ interim posteriors for tomographically binned galaxy subsamples \citep{Bonnett:16}.  
This paper is distinguished by its focus on metrics of photo-$z$ interim posteriors themselves and consideration of both classic and state-of-the-art photo-$z$ algorithms, comparing the performance of several of the most widely employed codes as well as some that have been developed only recently on the basis of metrics appropriate for a probabilistic data product.
The results presented in this work are
%part of the Data Challenge 1 (DC1)
a major focus of the Photometric Redshift working group of the LSST-DESC.
This work is laid out in the Science Roadmap (SRM)\footnote{Available at: \url{http://lsst-desc.org/sites/default/files/DESC_SRM_V1_1.pdf}} as one of the critical activities to be completed in preparation for dark energy science analysis on the first year LSST data.
This is the first of multiple papers by the working group, which will grow in sophistication.
In this initial paper we focus on evaluating the performance of photometric redshift codes and PDF-based performance metrics in the presence of representative training sets.
Comparing the relative performance of the codes enables us to evaluate whether each code is using information in an optimal way, and may reveal enhancements in some codes and deficiencies in others, either in the fundamental algorithm, or in specific implementation.  We note that these initial tests are a necessary requirement for photo-$z$ codes that will be used in cosmological analyses; however, meeting these requirements is only the first stage in the process of accounting for incomplete training samples, object blending, and other complications that realistic samples will encounter in estimating photometric redshifts.  This can be thought of as an initial test under near perfect conditions before further complexities are added in future papers.

Before moving forward, we must address how the best methods may be unique to the performance metrics and science cases considered and what distinguishes photo-$z$ PDFs of different methods from one another.
Though photo-$z$ PDFs are often written simply as $p(z)$, the PDF itself must be an interim posterior distribution $p(z | d, I)$, the probability of redshift conditioned on photometric data $d$ that has actually been observed and the prior information $I$ that guides how a redshift is extracted from the photometry.
If we run multiple photo-$z$ codes on a single dataset, the photo-$z$ interim posteriors will not be identical because each code is based on assumptions in the form of an interim prior --- these assumptions form the premise for photo-$z$ estimation as a whole and are the only way to introduce differences in estimates of what would otherwise be a shared photo-$z$ posterior $p(z | d)$ regardless of the code used to obtain it.
Though explicit knowledge of the interim prior is necessary to use photo-$z$ interim posteriors self-consistently in physical inference, the interim prior of a particular methodology is often implicit and not necessarily shared among all galaxies in the catalogue.

This paper therefore aims (1) to constrain the impact of the interim prior $I$ by separating it into a component $I_{H}$ representing the method itself and a component $I_{D}$ representing physical information, such as a training set or SED template library and (2) to present a procedure for evaluating the performance of photo-$z$ codes in generic tests that may include many more systematics in the interim prior $I$.
In order to isolate the effects encapsulated by $I_{H}$ of variation between codes from issues with the training set or template library encapsulated by $I_{D}$, we use an identical set of simulated galaxies for every code and construct a template library and training sample that are {\it complete and representative} and shared among all codes; that is, our training sample for machine learning codes is drawn from the same underlying galaxy population as our test set, with no additional selections, and the SED library used for template-based codes is the same as the one used to generate the photometric data.
We explore a number of performance metrics in this paper, not to make a conclusion regarding the superiority or even relative favorability of each code but to establish a method for comparing photo-$z$ PDFs derived by different methods.
These test conditions set the stage for addressing in a future paper the crucial issue of incomplete and non-representative prior information.

The outline of the paper is as follows: in \S\,\ref{sec:sims} we present the simulated data set; in \S\,\ref{sec:pzcodes} we describe the current generation codes employed in the paper;  in \S\,\ref{sec:metrics} we discuss the interpretation of photo-$z$ PDFs in terms of metrics of accuracy; in \S\,\ref{sec:results} we show our results and compare the performance of the codes; in \S\,\ref{sec:discussion} we offer our conclusions and discuss future extensions of this work.
