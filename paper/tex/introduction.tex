\section{Introduction}
\label{sec:intro}

The current and next generations of large-scale galaxy surveys, including the Dark Energy Survey \citep[\proj{DES},][]{Abbott:05}, the Kilo-Degree Survey \citep[\proj{KiDS},][]{de_Jong:13}, Hyper Suprime-Cam Survey \citep[\proj{HSC},][]{Aihara:2018a,Aihara:2018b}, \sout{Large Synoptic Survey Telescope} \boldblue{The Rubin Observatory Legacy Survey of Space and Time} \citep[\lsst,][]{Abell:09}, Euclid \citep{Laureijs:11}, and Wide-Field Infrared Survey Telescope \citep[\proj{WFIRST},][]{Green:12}, represent a paradigm shift to reliance on photometric, rather than solely spectroscopic, galaxy catalogues of substantially larger size at a cost of lacking complete spectroscopically confirmed redshifts ($z$).
Effective astrophysical inference using the catalogues resulting from these ongoing and upcoming missions, however, necessitates accurate and precise photometric redshift (\pz) estimation methodologies.

As an example, in order for \pz\ systematics to not dominate the statistical noise floor of \lsst's main cosmological sample of several $10^{9}$ galaxies, the \lsst\ Science Requirements Document (SRD)\footnote{available at \url{https://docushare.lsstcorp.org/docushare/dsweb/Get/LPM-17}} specifies that individual galaxy \pz s must have root-mean-square error $\sigma_z < 0.02 (1+z)$, $3 \sigma$ catastrophic outlier rate below $10$ per cent, and bias below $0.003$.
Specific science cases may have their own requirements on \pz\ performance that exceed those of the survey as a whole.
In that vein, the \lsst\ Dark Energy Science Collaboration (\lsstdesc) developed a separate SRD \citep{Mandelbaum:2018} that conservatively forecasts the constraining power of five cosmological probes, leading to even more stringent requirements on \pz\ performance, including those defined in terms of tomographically binned subsample populations\footnote{While tomographic samples will play a prominent role in some science cases, estimation of tomographic distributions is a distinct problem with distinct solutions. In this paper we focus solely on individual galaxy redshift estimates.} rather than individual galaxies.

Though the standard has long been for each galaxy in a photometric catalogue to have a \pz\ point estimate and Gaussian error bar, even early applications of \pz s in precision cosmology indicate the inadequacy of point estimates \citep{mandelbaum:2008} to encapsulate the degeneracies resulting from the nontrivial mapping between broad band fluxes and redshift.
Far from a hypothetical situation, such degeneracies are real consequences of the same deep imaging that enables larger galaxy catalogue sizes.
The lower luminosity and higher redshift populations captured by deeper imaging introduce major physical systematics to \pz s, among them the Lyman break/Balmer break degeneracy, that did not affect shallower large area surveys like the Sloan Digital Sky Survey \citep[\textsc{SDSS},][]{York:00} and Two Micron All Sky Survey \citep[\textsc{2MASS},][]{Skrutskie:06}.

To fully characterize such physical degeneracies, subsequent photometric galaxy catalogue data releases, \citep[e.~g.~][]{Sheldon:2012, Erben:2013, de_Jong:17}, provide a more informative \pz\ data product, the \pz\ probability density function (PDF), that describes the redshift probability, commonly denoted as $p(z)$, as a function of a galaxy's redshift, conditioned on the observed photometry.
Early template-based methods such as \citet{Fernandezsoto:99} approximated the likelihood of photometry conditioned on redshift with the relative \chisq\ values of template spectra.
Not long after, Bayesian adaptations of template-based approaches such as \citet{Benitez:00} combined the estimated likelihoods with a prior to yield a posterior PDF of redshift conditioned on photometry.
While the first data-driven \pz\ algorithms yielded a point estimate, \citet{Firth:03} estimated a \pzpdf\ using a neural net with realizations scattered within the photometric errors.

There are numerous techniques for deriving \pzpdf s, yet no one method has been established as clearly superior.
Consistent experimental conditions enable the quantification if not isolation of their differences, which can be interpreted as a sort of \textit{implicit prior} imparted by the method itself.
Comprehensive comparisons of \pz\ methods have been made before; the \Pz\ Accuracy And Testing \citep[\textsc{PHAT},][]{Hildebrandt:10} effort focused on \pz\ point estimates derived from many photometric bands.
\citet{Rau:2015} introduced a new method for improving \pzpdf s using an ordinal classification algorithm.
\textsc{DES} compared several codes for \pz\ point estimates and a subset with \pzpdf\ information \citep{Sanchez:14} and examined summary statistics of \pzpdf s for tomographically binned galaxy subsamples \citep{Bonnett:16}.

\boldblue{In currently available spectroscopic training sets systematic incompleteness, particularly at faint magnitudes, and incorrectly assigned redshifts are a harrowing problem.  Certain subpopulations can potentially fail to yield secure redshifts, given limited wavelength coverage.  Alternately, redshifts may be  incorrectly assigned for certain populations due to misidentification of spectral features.  These missing or misidentified populations would yield training sets that are not representative of the true underlying galaxy population, which can bias redshift estimates well beyond the levels required for upcoming analyses.  Characterizing and mitigating these problems will be a major undertaking which we will cover in future papers, as discussed briefly in \S,\ref{sec:futureexperiments}.  Before taking on this very complex issue, however, we must first investigate a more fundamental one: how do current methods perform in the presence of representative data?}

This paper is distinguished from other comparisons of \pz\ methods by its focus on the evaluation criteria for \pzpdf s and interpretation thereof.
In the absence of simulated data drawn from known redshift distributions, the very concept of a ``true PDF'' for an individual galaxy is unavailable, and we must instead rely on measures of ensemble behaviour to characterize PDF quality (see \S\,\ref{sec:metrics} for further discussion).
We aim to perform a comprehensive sensitivity analysis of the dependence of \pzpdf s on the code used to produce them in order to ultimately select those that will become part of the \lsstdesc\ pipelines, described in the Science Roadmap (SRM)\footnote{Available at: \url{https://lsstdesc.org/assets/pdf/docs/DESC_SRM_latest.pdf}}.
In this initial study, we focus on evaluating the performance of \pzpdf\ codes using PDF-specific performance metrics in a formally controlled experiment with complete and representative prior information (template libraries and training sets) to set a baseline for subsequent investigations.
This approach probes how each code considered exploits the information content of the data versus prior information from template libraries and training sets.

The outline of the paper is as follows: in \S\,\ref{sec:sims} we present the simulated data set; in \S\,\ref{sec:pzcodes} we describe the current generation codes employed in the paper; in \S\,\ref{sec:metrics} we discuss the interpretation of photo-$z$ PDFs in terms of metrics of accuracy; in \S\,\ref{sec:results} we show our results and compare the performance of the codes; in \S\,\ref{sec:discussion} we offer our conclusions and discuss future extensions of this work.
