\section{Introduction}
\label{sec:intro}

The current and next generations of large-scale galaxy surveys, including the Dark Energy Survey \citep[\textsc{DES},][]{Abbott:05}, the Kilo-Degree Survey \citep[\textsc{KiDS},][]{de_Jong:13}, Hyper Suprime-Cam Survey \citep[\textsc{HSC},][]{Aihara:2018a,Aihara:2018b}, Large Synoptic Survey Telescope \citep[\lsst,][]{Abell:09}, Euclid \citep{Laureijs:11}, and Wide-Field Infrared Survey Telescope \citep[\textsc{WFIRST},][]{Green:12}, present a paradigm shift from spectroscopic to photometric galaxy catalogs of substantially larger size at a cost of lacking complete spectroscopically confirmed redshifts.

Effective astrophysical inference using the catalogs resulting from these ongoing and upcoming missions, however, necessitates accurate and precise photometric redshift (\pz) estimation methodologies.
As an example, in order for \pz\ systematics to not dominate the statistical noise floor of \lsst's main cosmological sample of $\sim 10^{7}$ galaxies, the \lsst\ Science Requirements Document (SRD)\footnote{available at \url{https://docushare.lsstcorp.org/docushare/dsweb/Get/LPM-17}} specifies that individual galaxy \pz s must have root-mean-square error $\sigma_z < 0.02 (1+z)$, $3 \sigma$ ``catastrophic outlier'' rate below $10\%$, and bias below $0.003$.
Specific science cases may have their own requirements on \pz\ performance that exceed those of the survey as a whole.
In that vein, the \lsst\ Dark Energy Science Collaboration (\lsstdesc) developed a separate SRD \citep{Mandelbaum:2018} that conservatively forecasts the constraining power of five cosmological probes, leading to even more stringent requirements on \pz\ performance, including those defined in terms of tomographically binned subsamples populations rather than individual galaxies.

Though the standard has long been for each galaxy in a photometric catalog to have a \pz\ point estimate and Gaussian error bar, the nontrivial mapping between broad band fluxes and redshift renders this simplistic summary inadequate to quantify the uncertainty landscape by neglecting degenerate redshift solutions.
Far from a hypothetical situation, this degeneracy is a real consequence of the same deep imaging that enables larger galaxy catalog sizes.
The lower luminosity and higher redshift populations captured by deeper imaging introduce major physical systematics to \pz s, among them the Lyman break/Balmer break degeneracy, that did not affect shallower large area surveys like the Sloan Digital Sky Survey \citep[\textsc{SDSS},][]{York:00} and Two Micron All Sky Survey \citep[\textsc{2MASS},][]{Skrutskie:06}.

To fully characterize such physical degeneracies, photometric galaxy catalog data releases from \citep{Mandelbaum:2008} to \citep{de_Jong:17}, provided a more informative \pz\ data product, the \pz\ probability density function (PDF), that describes the relative probability as a function of a galaxy's redshift, conditioned on the observed photometry.
Early template-based methods such as \citet{Fernandezsoto:99} approximated the likelihood of photometry conditioned on redshift with the relative $\chi^{2}$ values of template spectra.
Not long after, Bayesian adaptations of template-based approaches such as \citet{Benitez:00} combined the estimated likelihoods with a prior to yield a posterior PDF of redshift conditioned on photometry.
While the first machine learning based algorithms focused on a point-estimate, \citet{Firth:03} estimated a \pzpdf\ using a neural net with 1000 realizations scattered within the photometric errors.

% MOVE TO WHERE \nz\ IS INTRODUCED AS A METRIC
% For cosmological measurements, certain science cases require redshift information on individual objects, e.~g.~identification of host galaxy redshift for supernova classification, or identifying potential cluster membership.
% Other science cases seem to need only ensemble redshift information; for instance many current cosmic shear techniques require only the overall redshift distribution $N(z)$ for tomographic redshift samples.
% However,  even such cases require individual object redshift estimates for portions of the analysis, for example in determining galaxy intrinsic alignments in weak lensing samples.
% In addition, recent data-driven techniques employing hierarchical Bayesian or Gaussian Process methods have emerged that calibrate redshift distributions using individual $p(z)$ estimates \citep[e.~g.~][]{Sanchez:2018}.
% Techniques for using \pzpdf s have lagged behind the development of codes to produce them, however, all existing approaches assume that the \pzpdf\ for each galaxy is an accurate PDF, with failures when the assumption is violated.
% Thus, even methods that seem to need only ensemble $N(z)$ may actually require accurate $p(z)$ in order to meet stringent survey requirements.
% Large photometric surveys such as LSST must develop algorithms that simultaneously meet the needs of all science cases.
% In order to meet these ambitious goals for photo-$z$ accuracy, every aspect of photo-$z$ estimation will have to be optimized: the algorithms employed, both template and machine-learning based (both in design and implementation); the spectroscopic data used as a training set for machine learning algorithms or to estimate template sets and train Bayesian priors; and probabilistic catalogue compression schemes that balance information retention against limited storage resources.

There are numerous techniques for deriving \pzpdf s, yet no one method has yet been established as clearly superior.
Quantitative comparisons of \pz\ methods have been made before.
The \Pz\ Accuracy And Testing \citep[\textsc{PHAT},][]{Hildebrandt:10} effort focused on \pz\ point estimates derived from many photometric bands.
\citet{Rau:2015} introduced a new method for improving \pzpdf s using an ordinal classification algorithm.
\textsc{DES} compared several codes for \pz\ point estimates and a subset with \pzpdf\ information \citep{Sanchez:14} and examined summary statistics of \pzpdf s for tomographically binned galaxy subsamples \citep{Bonnett:16}.

This paper is distinguished by its focus on the evaluation criteria for \pzpdf s and interpretation thereof, as part of the mission of the , as a key project of the Photometric Redshift working group of the \lsstdesc\ laid out in the Science Roadmap (SRM)\footnote{Available at: \url{http://lsst-desc.org/sites/default/files/DESC_SRM_V1_1.pdf}}.
In this initial study, we focus on evaluating the performance of \pzpdf\ codes and PDF-specific performance metrics in a controlled experiment with complete and representative prior information (template libraries and training sets).
% Specific implementation choices in each code will influence the resultant posterior distributions, for example choice of prior parameterization in template-based codes, the bandwidth size chosen for machine learning based codes, or even the output format chosen for storing the PDF.
% We have attempted to minimize the impact of many of these factors when comparing codes, for example by using the same template set for all template-based codes, and using a training set that is drawn from the same underlying population as the test sample, to create a controlled environment in which to compare the photo-$z$ PDFs derived from each method.
% We explore a number of performance metrics in this paper that test whether the posterior estimates are actual PDFs.
% Comparing the relative performance of the codes enables us to evaluate whether each code is using information in an optimal way, and may reveal enhancements in some codes and deficiencies in others, either in the fundamental algorithm, or in specific implementation.
% Identifying and fixing failure modes within codes may aid us in reaching the stringent photo-$z$ performance goals set out for LSST.
% We note that these initial tests are a necessary requirement for photo-$z$ codes that will be used in cosmological analyses; however, meeting these requirements is only the first stage in the process, and can be thought of as an initial test under near perfect conditions to test for problems before further complexities are added in future analyses.

The outline of the paper is as follows: in \S\,\ref{sec:sims} we present the simulated data set; in \S\,\ref{sec:pzcodes} we describe the current generation codes employed in the paper; in \S\,\ref{sec:metrics} we discuss the interpretation of photo-$z$ PDFs in terms of metrics of accuracy; in \S\,\ref{sec:results} we show our results and compare the performance of the codes; in \S\,\ref{sec:discussion} we offer our conclusions and discuss future extensions of this work.
