\section{Introduction}
\label{sec:intro}

The current and next generations of large-scale galaxy surveys, including the Dark Energy Survey \citep[\proj{DES},][]{Abbott:05}, the Kilo-Degree Survey \citep[\proj{KiDS},][]{de_Jong:13}, Hyper Suprime-Cam Survey \citep[\proj{HSC},][]{Aihara:2018a,Aihara:2018b}, Large Synoptic Survey Telescope \citep[\lsst,][]{Abell:09}, Euclid \citep{Laureijs:11}, and Wide-Field Infrared Survey Telescope \citep[\proj{WFIRST},][]{Green:12}, present a paradigm shift from spectroscopic to photometric galaxy catalogues of substantially larger size at a cost of lacking complete spectroscopically confirmed redshifts ($z$).

Effective astrophysical inference using the catalogues resulting from these ongoing and upcoming missions, however, necessitates accurate and precise photometric redshift (\pz) estimation methodologies.
As an example, in order for \pz\ systematics to not dominate the statistical noise floor of \lsst's main cosmological sample of $\sim 10^{7}$ galaxies, the \lsst\ Science Requirements Document (SRD)\footnote{available at \url{https://docushare.lsstcorp.org/docushare/dsweb/Get/LPM-17}} specifies that individual galaxy \pz s must have root-mean-square error $\sigma_z < 0.02 (1+z)$, $3 \sigma$ catastrophic outlier rate below $10\%$, and bias below $0.003$.
Specific science cases may have their own requirements on \pz\ performance that exceed those of the survey as a whole.
In that vein, the \lsst\ Dark Energy Science Collaboration (\lsstdesc) developed a separate SRD \citep{Mandelbaum:2018} that conservatively forecasts the constraining power of five cosmological probes, leading to even more stringent requirements on \pz\ performance, including those defined in terms of tomographically binned subsamples populations rather than individual galaxies.

% MOVE TO WHERE \nz\ IS INTRODUCED AS A METRIC
% For cosmological measurements, certain science cases require redshift information on individual objects, e.~g.~identification of host galaxy redshift for supernova classification, or identifying potential cluster membership.
% Other science cases seem to need only ensemble redshift information; for instance many current cosmic shear techniques require only the overall redshift distribution $N(z)$ for tomographic redshift samples.
% However,  even such cases require individual object redshift estimates for portions of the analysis, for example in determining galaxy intrinsic alignments in weak lensing samples.
% In addition, recent data-driven techniques employing hierarchical Bayesian or Gaussian Process methods have emerged that calibrate redshift distributions using individual $p(z)$ estimates \citep[e.~g.~][]{Sanchez:2018}.
% Techniques for using \pzpdf s have lagged behind the development of codes to produce them, however, all existing approaches assume that the \pzpdf\ for each galaxy is an accurate PDF, with failures when the assumption is violated.
% Thus, even methods that seem to need only ensemble $N(z)$ may actually require accurate $p(z)$ in order to meet stringent survey requirements.
% Large photometric surveys such as LSST must develop algorithms that simultaneously meet the needs of all science cases.
% In order to meet these ambitious goals for photo-$z$ accuracy, every aspect of photo-$z$ estimation will have to be optimized: the algorithms employed, both template and machine-learning based (both in design and implementation); the spectroscopic data used as a training set for machine learning algorithms or to estimate template sets and train Bayesian priors; and probabilistic catalogue compression schemes that balance information retention against limited storage resources.

Though the standard has long been for each galaxy in a photometric catalogue to have a \pz\ point estimate and Gaussian error bar, even early applications of \pz s in precision cosmology indicate the inadequacy of point estimates \citep{mandelbaum:2008} to encapsulate the degeneracies resulting from the nontrivial mapping between broad band fluxes and redshift.
Far from a hypothetical situation, this degeneracy is a real consequence of the same deep imaging that enables larger galaxy catalogue sizes.
The lower luminosity and higher redshift populations captured by deeper imaging introduce major physical systematics to \pz s, among them the Lyman break/Balmer break degeneracy, that did not affect shallower large area surveys like the Sloan Digital Sky Survey \citep[\textsc{SDSS},][]{York:00} and Two Micron All Sky Survey \citep[\textsc{2MASS},][]{Skrutskie:06}.

To fully characterize such physical degeneracies, later photometric galaxy catalogue data releases, (e.~g.~\citet{Erben:2013}, \citet{de_Jong:17}), provide a more informative \pz\ data product, the \pz\ probability density function (PDF), that describes the redshift probability, commonly denoted as $p(z)$, as a function of a galaxy's redshift, conditioned on the observed photometry.
Early template-based methods such as \citet{Fernandezsoto:99} approximated the likelihood of photometry conditioned on redshift with the relative \chisq\ values of template spectra.
Not long after, Bayesian adaptations of template-based approaches such as \citet{Benitez:00} combined the estimated likelihoods with a prior to yield a posterior PDF of redshift conditioned on photometry.
While the first data-driven \pz\ algorithms yielded a point estimate, \citet{Firth:03} estimated a \pzpdf\ using a neural net with realizations scattered within the photometric errors.

There are numerous techniques for deriving \pzpdf s, yet no one method has been established as clearly superior.
In fact, in the absence of simulated data drawn from known redshift distributions, the very concept of a ``true PDF'' for an individual galaxy is unavailable, and we must instead rely on measures of ensemble behaviour to characterize PDF quality (see \S\,\ref{sec:metrics} for further discussion).
This caveat aside, quantitative comparisons of \pz\ methods have been made before; the \Pz\ Accuracy And Testing \citep[\textsc{PHAT},][]{Hildebrandt:10} effort focused on \pz\ point estimates derived from many photometric bands.
\citet{Rau:2015} introduced a new method for improving \pzpdf s using an ordinal classification algorithm.
\textsc{DES} compared several codes for \pz\ point estimates and a subset with \pzpdf\ information \citep{Sanchez:14} and examined summary statistics of \pzpdf s for tomographically binned galaxy subsamples \citep{Bonnett:16}.

This paper is distinguished from other comparisons of \pz\ methods by its focus on the evaluation criteria for \pzpdf s and interpretation thereof.
We aim to perform a comprehensive sensitivity analysis of \pzpdf\ techniques in order to ultimately select those that will become part of the \lsst\ pipelines, as part of a key project of the Photometric Redshifts working group of the \lsstdesc, described in the Science Roadmap (SRM)\footnote{Available at: \url{http://lsst-desc.org/sites/default/files/DESC_SRM_V1_1.pdf}}.
In this initial study, we focus on evaluating the performance of \pzpdf\ codes using PDF-specific performance metrics in a controlled experiment with complete and representative prior information (template libraries and training sets) to set a baseline for subsequent investigations.
This approach probes how each code considered exploits the information content of the data versus prior information from template libraries and training sets.

The outline of the paper is as follows: in \S\,\ref{sec:sims} we present the simulated data set; in \S\,\ref{sec:pzcodes} we describe the current generation codes employed in the paper; in \S\,\ref{sec:metrics} we discuss the interpretation of photo-$z$ PDFs in terms of metrics of accuracy; in \S\,\ref{sec:results} we show our results and compare the performance of the codes; in \S\,\ref{sec:discussion} we offer our conclusions and discuss future extensions of this work.
