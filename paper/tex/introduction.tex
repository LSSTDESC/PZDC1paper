\section{Introduction}
\label{sec:intro}

%(Sam Schmidt, Ofer Lahav, Jeff Newman, Alex Malz, John Soo, Tony Tyson)

%\jan{Is the intention to keep these parenthetical author sets?  I find it a bit offputting in the text.  It might be better to put this for sections in the acknowledgements section, and for individual codes/statistics specify who ran/developed it with a sentence.}\red{I was just about to comment these out before finalizing the internal review draft of the paper. They are in place to remind me to contact a few people who still have not filled out their authors.csv info.}

Large-scale photometric galaxy surveys are entering a new era with currently or soon-to-be running Stage III and Stage IV dark energy experiments like the Dark Energy Survey \citep[DES,][]{Abbott:05}, the Kilo-Degree Survey \citep[KiDS,][]{de_Jong:13}, Large Synoptic Survey Telescope \citep[LSST,][]{Abell:09}, Euclid \citep{Laureijs:11}, and Wide-Field Infrared Survey Telescope \citep[WFIRST,][]{Green:12}.
The move to imaging based surveys, rather than spectroscopic based, for cosmological measurements makes proper understanding of photometric redshifts (``photo-$z$'s'') of paramount importance, as cosmological distance measures for statistical samples are directly dependent on photo-$z$ measurements.

The unprecedented sample size of LSST galaxies, expected to number several billion for the main cosmological sample, necessitates stringent constraints on photo-$z$ accuracy if systematic errors are not to dominate the statistical errors.
The LSST Science Requirements Document (SRD)\footnote{available at \url{https://docushare.lsstcorp.org/docushare/dsweb/Get/LPM-17}} lists the photometric redshift goals for a magnitude limited sample with $i<25$ as: root-mean-square error with a goal of $\sigma_z<0.02(1+z)$; $3\sigma$ ``catastrophic outlier'' rate below $10\%$; bias below $0.003$\footnote{
Note that at the time the SRD was written, these goals were stated in terms of a photo-$z$ point estimate for each galaxy, as was standard in many previous studies, while in this paper we emphasize the importance of using a full photo-$z$ PDF.}.

The tremendous size of LSST's galaxy catalog will be enabled by its exceptional depth, pushing to fainter magnitudes and deeper imaging and including galaxies of lower luminosity and higher redshift than ever before.
In addition to the contribution of low signal-to-noise photometry to the systematic error of photo-$z$s, these populations introduce major physical degeneracies, for example the Lyman break/Balmer break degeneracy, that were not present in the populations covered in previous large area surveys like the Sloan Digital Sky Survey \citep[SDSS,][]{York:00} and the Two Micron All Sky Survey \citep[2MASS,][]{Skrutskie:06}.
In order to meet LSST's demanding error budget, it will be necessary to fully characterize those degeneracies wherein multiple redshift solutions have comparable likelihood.

There is often a desire to have a single valued ``point-estimate'' redshift for an individual galaxy.  However, the complex, non-linear (and often non-unique) nature of the mapping between broad band fluxes and redshift means that a single value is unable to capture the full redshift information encoded in a galaxy's magnitudes.  For example, a common point-estimate for a template-based method is taking the highest likelihood solution as the point photo-$z$.  A single valued redshift ignores degenerate redshift solutions of lower probability, potentially biasing photometric redshift estimates both for individual galaxies and ensemble distributions.  Storing more information is necessary, most often photo-$z$ codes output the redshift probability density function (PDF), also often referred to as $p(z)$, describing the relative likelihood as a function of redshift. %\red{should we move some of AIM's p(z) vs p(z|d,I) to here?}
Early template methods such as \citet{Fernandezsoto:99} converted relative $\chi^2$ values of template spectra to likelihoods to estimate $p(z)$.
Soon after, codes such as \citet{Benitez:00} added a Bayesian prior and output a posterior probability distribution.
While many early machine learning based algorithms focused on a point-estimate, \citet{Firth:03} used a neural net with 1000 realizations scattered within the photometric errors to estimate a $p(z)$.
As more groups began to employ photometric redshifts in their cosmological analyses, realization that point-estimate photo-$z$'s were inadequate for precision cosmology measurements \citep{Mandelbaum:2008}.
From around this point onward, most photo-$z$ algorithms have attempted to implement some estimate of the overall redshift probability in their outputs, and some surveys began supplying a full $p(z)$ rather than a simple redshift point-estimate and error \citep[e.~g.~][]{de_Jong:17}.

%\aim{We should spend a whole paragraph establishing that photo-$z$ PDFs are a thing, i.e. review the motivation for PDFs over point estimates, basic definition, early methods papers, surveys that have published them, etc.}
%\scc{I agree with Alex, we should introduce here what a pdf is and which survey have already published some pdfs (KiDS dr3 for example) or will produce them (LSST, Euclid etc.)}
%\red{See above paragraph, does this adequately address this?--SJS}\scc{fine for me}

There are numerous techniques for deriving photo-$z$ PDFs from photometry, yet no one method has yet been established as clearly superior.
Quantitative comparisons of photo-$z$ methods have been made before.
The Photo-$z$ Accuracy And Testing \citep[PHAT,][]{Hildebrandt:10} effort focused on point estimates derived from many photometric bands.
DES compared several codes for point estimates \citep{Sanchez:14} and a summary statistic of photo-$z$ interim posteriors for tomographically binned galaxy subsamples \citep{Bonnett:16}.
This paper is distinguished by its inclusion of metrics of photo-$z$ interim posteriors themselves and consideration of both classic and state-of-the-art photo-$z$ algorithms, comparing the performance of several of the most widely employed codes as well as some that have been developed only recently on the basis of metrics appropriate for a probabilistic data product.
The results presented in this work are
%part of the Data Challenge 1 (DC1)
a major focus of the Photometric Redshift working group of the LSST Dark Energy Science Collaboration (LSST-DESC).
This work is laid out in the Science Roadmap (SRM)\footnote{Available at: \url{http://lsst-desc.org/sites/default/files/DESC_SRM_V1_1.pdf}} as one of the critical activities to be completed in preparation for dark energy science analysis on the first year LSST data.
This is the first of multiple papers by the working group, which will grow in sophistication.
In this initial paper we focus on evaluating the performance of photometric redshift codes and their ability to produce accurate PDFs in the presence of representative training sets.
This can be thought of as an initial test under near perfect conditions, before further complexities are added in future papers.
Comparing the relative performance of the codes enables us to evaluate whether each code is using information in an optimal way, and may reveal enhancements in some codes and deficiencies in others, either in the fundamental algorithm, or in specific implementation.

Certain science cases need redshift information on individual objects, e.~g.~identification of host galaxy redshift for supernova classification, or identifying potential cluster membership.
Other science cases need only ensemble redshift information; for instance current weak lensing techniques require the overall redshift distribution $N(z)$ for tomographic redshift samples, but do not need single galaxy estimates.
In the case of the multiple types of probes of cosmology enabled by the
LSST cosmology sample of several billion galaxies, the number of redshift bins and their photo-$z$ requirements vary with the specific probe;  2-point angular correlations benefit from many bins, while weak lensing probes do not (due to the wide lensing kernel).
Large photometric surveys such as LSST must develop algorithms that meet the needs of all such science cases.
In order to meet these ambitious goals for photo-$z$ accuracy, every aspect of photo-$z$ estimation will have to be optimized: the algorithms employed, both template and machine-learning based (both in design and implementation); the spectroscopic data used as a training set for machine learning algorithms or to estimate template sets and train Bayesian priors; and probabilistic catalog compression schemes that balance information retention against limited storage resources.

Before moving forward, we must address how the best methods may be unique to the performance metrics and science cases considered and what distinguishes photo-$z$ PDFs of different methods from one another.
Though photo-$z$ PDFs are often written simply as $p(z)$, the PDF itself must be an interim posterior distribution $p(z | d, I)$, the probability of redshift conditioned on photometric data $d$ that has actually been observed and the prior information $I$ that guides how a redshift is extracted from the photometry.
If we run multiple photo-$z$ codes on a single dataset, the photo-$z$ interim posteriors will not be identical because each code is based on assumptions in the form of an interim prior --- these assumptions form the premise for photo-$z$ estimation as a whole and are the only way to introduce differences in estimates of what would otherwise be a shared photo-$z$ posterior $p(z | d)$ regardless of the code used to obtain it.
Though explicit knowledge of the interim prior is necessary to use photo-$z$ interim posteriors self-consistently in physical inference, the interim prior of a particular methodology is often implicit and not necessarily shared among all galaxies in the catalog.

This paper therefore aims (1) to constrain the impact of the interim prior $I$ by separating it into a component $I_{H}$ representing the method itself and a component $I_{D}$ representing physical information, such as a training set or SED template library and (2) to present a procedure for evaluating the performance of photo-$z$ codes in generic tests that may include many more systematics in the interim prior $I$.
In order to isolate the effects encapsulated by $I_{H}$ of variation between codes from issues with the training set or template library encapsulated by $I_{D}$, we use an identical set of simulated galaxies for every code and construct a template library and training sample that are {\it complete and representative} and shared among all codes; that is, our training sample for machine learning codes is drawn from the same underlying galaxy population as our test set, with no additional selections, and the SED library used for template-based codes is the same as the one used to generate the photometric data.
We explore a number of performance metrics in this paper, not to make a conclusion regarding the superiority or even relative favorability of each code but to establish a method for comparing photo-$z$ PDFs derived by different methods.
These test conditions set the stage for addressing in a future paper the crucial issue of incomplete and non-representative prior information.

The outline of the paper is as follows: in \S\,\ref{sec:sims} we present the simulated data set; in \S\,\ref{sec:pzcodes} we describe the current generation codes employed in the paper;  in \S\,\ref{sec:metrics} we discuss the interpretation of photo-$z$ PDFs in terms of metrics of accuracy; in \S\,\ref{sec:results} we show our results and compare the performance of the codes; in \S\,\ref{sec:discussion} we offer our conclusions and discuss future extensions of this work.
